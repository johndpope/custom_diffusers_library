{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OAJJBpEO8tH"
      },
      "source": [
        "# Temporal unet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wisleythewise/custom_diffusers_library.git\n",
        "%cd /content/custom_diffusers_library\n",
        "!pip install -e .\n",
        "!pip install transformers accelerate\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e8a68c410ded4350a845be8b8a5b615e",
            "c270ca0f5d3049a1ab485e7989548631",
            "73ccace5c44d4a6dacd6a61c5c2b5fb6",
            "623a3f6716e0470c9eab6cf7324fe464",
            "9d1b1fb54ce04956a8d099ff79e158ae",
            "8a68d1ac3d7644cc97ae1550f0c43414",
            "380df1a25b8e46b6b2c0e0b638692362",
            "76411395febb4cf2bb668bb05c7555d9",
            "da548536353640e39fe84966f56a1b4d",
            "ebbd3b535a10443badb4e029524c7997",
            "b431df09e6334fe0897dc321dfe82f78"
          ]
        },
        "id": "1ztDtdRQxipA",
        "outputId": "d72f1029-d36c-491e-b5f5-775919e8c4ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'custom_diffusers_library' already exists and is not an empty directory.\n",
            "/content/custom_diffusers_library\n",
            "Obtaining file:///content/custom_diffusers_library\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (7.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.26.0.dev0) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.26.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.26.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.26.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.26.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.26.0.dev0) (23.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.26.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.26.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.26.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.26.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.26.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.26.0.dev0-0.editable-py3-none-any.whl size=11096 sha256=ffbcd1ce41fc57f895de7afe5ee720d89a4da11cf9e0a1bae3c3819e0368c00f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xaarchx7/wheels/f6/06/4f/ee79f1ccb98c1c1d7c14fc34a8cf908c7f73d80d46952161c2\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.26.0.dev0\n",
            "    Uninstalling diffusers-0.26.0.dev0:\n",
            "      Successfully uninstalled diffusers-0.26.0.dev0\n",
            "Successfully installed diffusers-0.26.0.dev0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8a68c410ded4350a845be8b8a5b615e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L0rr3gE8O8tI"
      },
      "outputs": [],
      "source": [
        "import debugpy\n",
        "import gc\n",
        "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
        "from typing import Optional, Tuple, Union\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "from diffusers.utils import load_image, export_to_video\n",
        "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
        "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
        "from diffusers.utils import BaseOutput, logging\n",
        "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
        "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
        "from diffusers.models.modeling_utils import ModelMixin\n",
        "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
        "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
        "\n",
        "\n",
        "from types import MethodType\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/custom_diffusers_library')\n",
        "\n",
        "# Now try importing your custom package\n",
        "from diffusers import StableVideoDiffusionPipeline"
      ],
      "metadata": {
        "id": "Ry5RrvqDcHbF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "a38a4596231e444bb6c677a8653d0b34",
            "3b3d3740c5114152910dbd3a5b5f3e23",
            "582f087b56fe4f88bbed0b396d9a7248",
            "90bea31d332f45818ae0c32c654b46e8",
            "d730b85a76a947a087f9293dfa02fde4",
            "34516a7f5304436caba0906477c4ec5e",
            "553d5482a74b4aea9b62e0b0277126e8",
            "45731d5bccdb4317a6deabc5686b712b",
            "8adc2843531b41eeb4d5be3158cbe230",
            "17c07b19e3be41ea89ce2228c25ef88d",
            "f980c39e3d1e44fbb84d56859eeb8964"
          ]
        },
        "id": "zk7SjVsfO8tJ",
        "outputId": "221c34d6-520f-43e2-8bf6-17eac32b8f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Keyword arguments {'num_frames': 2} are not expected by StableVideoDiffusionPipeline and will be ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a38a4596231e444bb6c677a8653d0b34"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\", num_frames = 2\n",
        ")\n",
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU2ZICVVO8tJ",
        "outputId": "45db798b-d856-4655-c608-3c6c068bf98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FrozenDict([('vae', ('diffusers', 'AutoencoderKLTemporalDecoder')), ('image_encoder', ('transformers', 'CLIPVisionModelWithProjection')), ('unet', ('diffusers', 'UNetSpatioTemporalConditionModel')), ('scheduler', ('diffusers', 'EulerDiscreteScheduler')), ('feature_extractor', ('transformers', 'CLIPImageProcessor')), ('_name_or_path', 'stabilityai/stable-video-diffusion-img2vid-xt')])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "pipe_config = pipe.config\n",
        "print(pipe_config)\n",
        "unet_weights = pipe.unet.state_dict()\n",
        "my_net = UNetSpatioTemporalConditionModel()\n",
        "my_net.load_state_dict(unet_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLlHpSyhO8tJ",
        "outputId": "f920277f-56fd-4f41-837c-8111a7796428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latent model input dtype: torch.float16\n",
            "Hidden image embeddings dtype: torch.float16\n",
            "torch.Size([2, 1, 8, 64, 64])\n",
            "torch.Size([2, 1, 1024])\n",
            "torch.Size([2, 3])\n",
            "Latent model input is on: cuda:0\n",
            "Hidden image embeddings are on: cuda:0\n",
            "Added time IDs are on: cuda:0\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "def prepare_latents(\n",
        "    batch_size,\n",
        "    num_frames,\n",
        "    num_channels_latents,\n",
        "    height,\n",
        "    width,\n",
        "    dtype,\n",
        "    device,\n",
        "    generator,\n",
        "    latents=None,\n",
        "):\n",
        "    shape = (\n",
        "        batch_size,\n",
        "        num_frames,\n",
        "        num_channels_latents // 2,\n",
        "        height // 1,\n",
        "        width // 1,\n",
        "    )\n",
        "    if isinstance(generator, list) and len(generator) != batch_size:\n",
        "        raise ValueError(\n",
        "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
        "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
        "        )\n",
        "\n",
        "    if latents is None:\n",
        "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
        "    else:\n",
        "        latents = latents.to(device)\n",
        "\n",
        "    # scale the initial noise by the standard deviation required by the scheduler\n",
        "    latents = latents * 0.2\n",
        "    return latents\n",
        "\n",
        "def pseudo_image_embeddings( shape, generator, device, dtype, do_classifier_free_guidance = True ):\n",
        "    image_embeddings = randn_tensor(shape, generator=generator, device= device, dtype=dtype)\n",
        "\n",
        "    if do_classifier_free_guidance:\n",
        "        negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
        "\n",
        "        # For classifier free guidance, we need to do two forward passes.\n",
        "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "        # to avoid doing two forward passes\n",
        "        return torch.cat([negative_image_embeddings, image_embeddings])\n",
        "\n",
        "def get_add_time_ids(\n",
        "  fps = 7,\n",
        "  motion_bucket_id = 127,\n",
        "  noise_aug_strength = 0.02,\n",
        "  dtype = torch.float32,\n",
        "  batch_size = 1,\n",
        "  num_videos_per_prompt = 1,\n",
        "  do_classifier_free_guidance = True,\n",
        "):\n",
        "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
        "\n",
        "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
        "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
        "\n",
        "  if do_classifier_free_guidance:\n",
        "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
        "\n",
        "  return add_time_ids\n",
        "\n",
        "dtype = torch.float16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
        "\n",
        "# Generate original latents with specified dtype\n",
        "my_latents = prepare_latents(1, 1, 8, 64, 64, dtype, device, generator)\n",
        "\n",
        "# Apply classifier-free guidance by duplicating the latents and ensuring the correct dtype\n",
        "latent_model_input = torch.cat([my_latents] * 2)  # Inherits dtype from my_latents\n",
        "\n",
        "# Create pseudo image latents by cloning the original latents\n",
        "pseudo_image_latents = latent_model_input.clone()  # Inherits dtype\n",
        "\n",
        "# Concatenate pseudo image latents over the channels dimension, ensuring matching dtype\n",
        "latent_model_input = torch.cat([latent_model_input, pseudo_image_latents], dim=2)\n",
        "\n",
        "\n",
        "# Create the fake image embeddings with the specified dtype\n",
        "hidden_image_embeddings = pseudo_image_embeddings((1, 1, 1024), generator, device, dtype)\n",
        "\n",
        "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
        "\n",
        "# Verify the dtype of both tensors\n",
        "print(f\"Latent model input dtype: {latent_model_input.dtype}\")\n",
        "print(f\"Hidden image embeddings dtype: {hidden_image_embeddings.dtype}\")\n",
        "\n",
        "print(latent_model_input.shape)\n",
        "print(hidden_image_embeddings.shape)\n",
        "print(added_time_ids.shape)\n",
        "\n",
        "# Print on which model they are\n",
        "print(f\"Latent model input is on: {latent_model_input.device}\")\n",
        "print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
        "# Assuming added_time_ids is also a tensor; replace this with the actual tensor variable if different\n",
        "print(f\"Added time IDs are on: {added_time_ids.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtype = torch.float16\n",
        "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TmG-GmZVRLMn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUyIR4ykO8tJ",
        "outputId": "a11e18c8-b7eb-4e10-86fd-9ca945bee96f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNetSpatioTemporalConditionModel(\n",
              "  (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (time_proj): Timesteps()\n",
              "  (time_embedding): TimestepEmbedding(\n",
              "    (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "  )\n",
              "  (add_time_proj): Timesteps()\n",
              "  (add_embedding): TimestepEmbedding(\n",
              "    (linear_1): LoRACompatibleLinear(in_features=768, out_features=1280, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "  )\n",
              "  (down_blocks): ModuleList(\n",
              "    (0): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=640, out_features=2560, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): DownBlockSpatioTemporal(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_blocks): ModuleList(\n",
              "    (0): UpBlockSpatioTemporal(\n",
              "      (resnets): ModuleList(\n",
              "        (0-2): 3 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 2560, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): CrossAttnUpBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 2560, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (2): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1920, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): CrossAttnUpBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=640, out_features=2560, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1920, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (2): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 960, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): CrossAttnUpBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 960, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1-2): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mid_block): UNetMidBlockSpatioTemporal(\n",
              "    (attentions): ModuleList(\n",
              "      (0): TransformerSpatioTemporalModel(\n",
              "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (transformer_blocks): ModuleList(\n",
              "          (0): BasicTransformerBlock(\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn1): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn2): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (temporal_transformer_blocks): ModuleList(\n",
              "          (0): TemporalBasicTransformerBlock(\n",
              "            (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff_in): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn1): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn2): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (time_pos_embed): TimestepEmbedding(\n",
              "          (linear_1): LoRACompatibleLinear(in_features=1280, out_features=5120, bias=True)\n",
              "          (act): SiLU()\n",
              "          (linear_2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "        (time_proj): Timesteps()\n",
              "        (time_mixer): AlphaBlender()\n",
              "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (resnets): ModuleList(\n",
              "      (0-1): 2 x SpatioTemporalResBlock(\n",
              "        (spatial_res_block): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (temporal_res_block): TemporalResnetBlock(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (time_mixer): AlphaBlender()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "  (conv_act): SiLU()\n",
              "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# my_net = my_net.half()\n",
        "my_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dcDj_n-pO8tK"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIG8TrRTO8tK",
        "outputId": "a1864cf1-1eae-43f5-a4dd-28ca5cb2eb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the batch size 2\n",
            "This is the iteration 0\n",
            "This is the iteration 1\n",
            "This is the iteration 2\n",
            "This is the iteration 3\n",
            "torch.Size([2, 1, 4, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    noise_pred = my_net.forward(\n",
        "        latent_model_input.to(dtype=dtype),\n",
        "        torch.tensor(1).to(dtype=dtype, device=device),\n",
        "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
        "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
        "        down_block_additional_residuals= None,\n",
        "        mid_block_additional_residual = None,\n",
        "        return_dict=False,\n",
        "    )[0]\n",
        "\n",
        "    print(noise_pred.shape)\n",
        "    if noise_pred is not None:\n",
        "        del noise_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is-g1cJbO8tK"
      },
      "source": [
        "# Controlnet Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7oWmnBK5O8tK"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, Union, Dict, Any\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomConditioningNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        # Initial convolution to match the first target channel dimension\n",
        "        self.initial_conv = nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Defining a series of convolutional blocks to progressively downsample\n",
        "        # and increase channel dimensions towards the target size\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "                nn.SiLU()\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "                nn.SiLU()\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 320, kernel_size=3, stride=2, padding=1),\n",
        "                nn.SiLU()\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Final adjustment to target spatial dimensions\n",
        "        # Considering a final adaptive pooling layer to ensure matching to the target spatial size (64x64)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((72, 128))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x = self.initial_conv(x)\n",
        "\n",
        "        for block in self.conv_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.adaptive_pool(x)\n",
        "\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        x = torch.cat([x,x])\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class UNetSpatioTemporalConditionOutput(BaseOutput):\n",
        "    \"\"\"\n",
        "    The output of [`UNetSpatioTemporalConditionModel`].\n",
        "\n",
        "    Args:\n",
        "        sample (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
        "            The hidden states output conditioned on `encoder_hidden_states` input. Output of last layer of model.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    sample: torch.FloatTensor = None\n",
        "\n",
        "\n",
        "class SpatioTemporalControlNetOutput(BaseOutput):\n",
        "    \"\"\"\n",
        "    The output of [`ControlNetModel`].\n",
        "\n",
        "    Args:\n",
        "        down_block_res_samples (`tuple[torch.Tensor]`):\n",
        "            A tuple of downsample activations at different resolutions for each downsampling block. Each tensor should\n",
        "            be of shape `(batch_size, channel * resolution, height //resolution, width // resolution)`. Output can be\n",
        "            used to condition the original UNet's downsampling activations.\n",
        "        mid_down_block_re_sample (`torch.Tensor`):\n",
        "            The activation of the midde block (the lowest sample resolution). Each tensor should be of shape\n",
        "            `(batch_size, channel * lowest_resolution, height // lowest_resolution, width // lowest_resolution)`.\n",
        "            Output can be used to condition the original UNet's middle block activation.\n",
        "    \"\"\"\n",
        "\n",
        "    down_block_res_samples: Tuple[torch.Tensor]\n",
        "    mid_block_res_sample: torch.Tensor\n",
        "\n",
        "    # Add a class which prints the sizes of the tensors\n",
        "    def print_sizes(self):\n",
        "        print(f\"Down block res samples: {self.down_block_res_samples[0].shape}\")\n",
        "        print(f\"Mid block res sample: {self.mid_block_res_sample.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SpatioTemporalControlNet(ModelMixin, ConfigMixin):\n",
        "    \"\"\"\n",
        "    A SpatioTemporalControlNet model for conditioning on spatio-temporal data.\n",
        "    This model adapts concepts from both ControlNetModel and UNetSpatioTemporalConditionModel,\n",
        "    focusing on handling video frames over time.\n",
        "    \"\"\"\n",
        "\n",
        "    _supports_gradient_checkpointing = True\n",
        "\n",
        "    @register_to_config\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_size: Optional[int] = None,\n",
        "        in_channels: int = 8,\n",
        "        down_block_types: Tuple[str] = (\n",
        "            \"CrossAttnDownBlockSpatioTemporal\",\n",
        "            \"CrossAttnDownBlockSpatioTemporal\",\n",
        "            \"CrossAttnDownBlockSpatioTemporal\",\n",
        "            \"DownBlockSpatioTemporal\",\n",
        "        ),\n",
        "        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n",
        "        addition_time_embed_dim: int = 256,\n",
        "        projection_class_embeddings_input_dim: int = 768,\n",
        "        layers_per_block: Union[int, Tuple[int]] = 2,\n",
        "        cross_attention_dim: Union[int, Tuple[int]] = 1024,\n",
        "        transformer_layers_per_block: Union[int, Tuple[int], Tuple[Tuple]] = 1,\n",
        "        num_attention_heads: Union[int, Tuple[int]] = (5, 10, 10, 20),\n",
        "        conditioning_embedding = None\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.sample_size = sample_size\n",
        "        self.conditioning_embedding = conditioning_embedding\n",
        "\n",
        "        # input\n",
        "        self.conv_in = nn.Conv2d(\n",
        "            in_channels,\n",
        "            block_out_channels[0],\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "\n",
        "        # time\n",
        "        time_embed_dim = block_out_channels[0] * 4\n",
        "\n",
        "        self.time_proj = Timesteps(block_out_channels[0], True, downscale_freq_shift=0)\n",
        "        timestep_input_dim = block_out_channels[0]\n",
        "\n",
        "        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
        "\n",
        "        self.add_time_proj = Timesteps(addition_time_embed_dim, True, downscale_freq_shift=0)\n",
        "        self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)\n",
        "\n",
        "\n",
        "        output_channel = block_out_channels[0]\n",
        "\n",
        "        self.controlnet_down_blocks = None\n",
        "        self.down_blocks = nn.ModuleList([])\n",
        "\n",
        "\n",
        "\n",
        "        if isinstance(num_attention_heads, int):\n",
        "            num_attention_heads = (num_attention_heads,) * len(down_block_types)\n",
        "\n",
        "        if isinstance(cross_attention_dim, int):\n",
        "            cross_attention_dim = (cross_attention_dim,) * len(down_block_types)\n",
        "\n",
        "        if isinstance(layers_per_block, int):\n",
        "            layers_per_block = [layers_per_block] * len(down_block_types)\n",
        "\n",
        "        if isinstance(transformer_layers_per_block, int):\n",
        "            transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n",
        "\n",
        "        blocks_time_embed_dim = time_embed_dim\n",
        "\n",
        "        # Initialize the connection between the down blocks and the unet\n",
        "        output_channel = block_out_channels[0]\n",
        "\n",
        "        # controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
        "        # controlnet_block = zero_module(controlnet_block)\n",
        "        # self.controlnet_down_blocks.append(controlnet_block)\n",
        "\n",
        "        # down\n",
        "        output_channel = block_out_channels[0]\n",
        "        for i, down_block_type in enumerate(down_block_types):\n",
        "            input_channel = output_channel\n",
        "            output_channel = block_out_channels[i]\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            down_block = get_down_block(\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                temb_channels=blocks_time_embed_dim,\n",
        "                num_layers=layers_per_block[i],\n",
        "                transformer_layers_per_block=transformer_layers_per_block[i],\n",
        "                add_downsample= not is_final_block,\n",
        "                resnet_eps=1e-5,\n",
        "                down_block_type=down_block_type,\n",
        "                cross_attention_dim=cross_attention_dim[i],\n",
        "                num_attention_heads=num_attention_heads[i],\n",
        "                resnet_act_fn=\"silu\",\n",
        "            )\n",
        "            self.down_blocks.append(down_block)\n",
        "\n",
        "            # for _ in range(layers_per_block[i]):\n",
        "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
        "            #     controlnet_block = zero_module(controlnet_block)\n",
        "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
        "\n",
        "            # if not is_final_block:\n",
        "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
        "            #     controlnet_block = zero_module(controlnet_block)\n",
        "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # hardcoded_controlnet_block_dims = [320,320, 640,640, 1280, 1280, 1280,1280,1280]\n",
        "        # for index, controlnet_block_dim in enumerate(hardcoded_controlnet_block_dims):\n",
        "        #     controlnet_block = nn.Conv2d(controlnet_block_dim, controlnet_block_dim, kernel_size=1)\n",
        "        #     controlnet_block = zero_module(controlnet_block)\n",
        "        #     self.controlnet_down_blocks.append(controlnet_block)\n",
        "\n",
        "\n",
        "        # Connections for the mid block\n",
        "        mid_block_channel = block_out_channels[-1]\n",
        "\n",
        "        controlnet_block = nn.Conv2d(mid_block_channel, mid_block_channel, kernel_size=1)\n",
        "        controlnet_block = zero_module(controlnet_block)\n",
        "        self.controlnet_mid_block = controlnet_block\n",
        "\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlockSpatioTemporal(\n",
        "            block_out_channels[-1],\n",
        "            temb_channels=blocks_time_embed_dim,\n",
        "            transformer_layers_per_block=transformer_layers_per_block[-1],\n",
        "            cross_attention_dim=cross_attention_dim[-1],\n",
        "            num_attention_heads=num_attention_heads[-1],\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sample: torch.FloatTensor,\n",
        "        timestep: Union[torch.Tensor, float, int],\n",
        "        encoder_hidden_states: torch.Tensor,\n",
        "        added_time_ids: torch.Tensor,\n",
        "        return_dict: bool = True,\n",
        "        controlnet_condition : torch.FloatTensor = None,\n",
        "    ) -> Union[UNetSpatioTemporalConditionOutput, Tuple]:\n",
        "        r\"\"\"\n",
        "        The [`UNetSpatioTemporalConditionModel`] forward method.\n",
        "\n",
        "        Args:\n",
        "            sample (`torch.FloatTensor`):\n",
        "                The noisy input tensor with the following shape `(batch, num_frames, channel, height, width)`.\n",
        "            timestep (`torch.FloatTensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
        "            encoder_hidden_states (`torch.FloatTensor`):\n",
        "                The encoder hidden states with shape `(batch, sequence_length, cross_attention_dim)`.\n",
        "            added_time_ids: (`torch.FloatTensor`):\n",
        "                The additional time ids with shape `(batch, num_additional_ids)`. These are encoded with sinusoidal\n",
        "                embeddings and added to the time embeddings.\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] instead of a plain\n",
        "                tuple.\n",
        "        Returns:\n",
        "            [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] or `tuple`:\n",
        "                If `return_dict` is True, an [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] is returned, otherwise\n",
        "                a `tuple` is returned where the first element is the sample tensor.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            # This would be a good case for the `match` statement (Python 3.10+)\n",
        "            is_mps = sample.device.type == \"mps\"\n",
        "            if isinstance(timestep, float):\n",
        "                dtype = torch.float32 if is_mps else torch.float64\n",
        "            else:\n",
        "                dtype = torch.int32 if is_mps else torch.int64\n",
        "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
        "        elif len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        batch_size, num_frames = sample.shape[:2]\n",
        "        timesteps = timesteps.expand(batch_size)\n",
        "\n",
        "        t_emb = self.time_proj(timesteps)\n",
        "\n",
        "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
        "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
        "        # there might be better ways to encapsulate this.\n",
        "\n",
        "        sample = sample.to(dtype=torch.float32)\n",
        "        # Set this to half\n",
        "\n",
        "        t_emb = t_emb.to(dtype=sample.dtype)\n",
        "\n",
        "        emb = self.time_embedding(t_emb)\n",
        "\n",
        "        time_embeds = self.add_time_proj(added_time_ids.flatten())\n",
        "        time_embeds = time_embeds.reshape((batch_size, -1))\n",
        "        time_embeds = time_embeds.to(emb.dtype)\n",
        "        aug_emb = self.add_embedding(time_embeds)\n",
        "        emb = emb + aug_emb\n",
        "\n",
        "        # Flatten the batch and frames dimensions\n",
        "        # sample: [batch, frames, channels, height, width] -> [batch * frames, channels, height, width]\n",
        "        sample = sample.flatten(0, 1)\n",
        "\n",
        "\n",
        "        # Repeat the embeddings num_video_frames times\n",
        "        # emb: [batch, channels] -> [batch * frames, channels]\n",
        "        emb = emb.repeat_interleave(num_frames, dim=0).to(sample.device)\n",
        "        # encoder_hidden_states: [batch, 1, channels] -> [batch * frames, 1, channels]\n",
        "        # Let encoder_hidden_states be just zeros in the correct format\n",
        "        # shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
        "\n",
        "        if encoder_hidden_states is None:\n",
        "            shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
        "            encoder_hidden_states = torch.zeros(shape_encoder_hidden_states, device=sample.device).repeat_interleave(num_frames, dim=0).to(dtype=sample.dtype)\n",
        "            print(f\"Shape of encoder hidden states without: {encoder_hidden_states.shape}\")\n",
        "        else:\n",
        "            encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=0)\n",
        "            print(f\"Shape of encoder hidden states with: {encoder_hidden_states.shape}\")\n",
        "\n",
        "        # Print the shape of the sample\n",
        "        # 2. pre-process\n",
        "        print(f\"Sample shape before the conversion: {sample.shape}\")\n",
        "        sample = self.conv_in(sample)\n",
        "        print(f\"Sample shape after the conversion: {sample.shape}\")\n",
        "\n",
        "\n",
        "        if controlnet_condition is not None:\n",
        "            # Check if it has the same shape as the sample otherwise error\n",
        "            # To the device of the sample\n",
        "            controlnet_condition = controlnet_condition.to(dtype=sample.dtype, device = sample.device)\n",
        "            controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n",
        "            controlnet_condition = controlnet_condition.flatten(0, 1)\n",
        "            if controlnet_condition.shape != sample.shape:\n",
        "                raise ValueError(f\"Jappie Controlnet condition shape {controlnet_condition.shape} does not match the sample shape {sample.shape}\")\n",
        "        else:\n",
        "            controlnet_condition = torch.zeros_like(sample).to(sample.device, dtype=sample.dtype)\n",
        "\n",
        "        sample += controlnet_condition\n",
        "\n",
        "\n",
        "        image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)\n",
        "\n",
        "        down_block_res_samples = (sample,)\n",
        "\n",
        "        for downsample_block in self.down_blocks:\n",
        "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
        "                sample, res_samples = downsample_block(\n",
        "                    hidden_states=sample,\n",
        "                    temb=emb,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    image_only_indicator=image_only_indicator,\n",
        "                )\n",
        "            else:\n",
        "                sample, res_samples = downsample_block(\n",
        "                    hidden_states=sample,\n",
        "                    temb=emb,\n",
        "                    image_only_indicator=image_only_indicator,\n",
        "                )\n",
        "            # Print the shapes of the res_samples\n",
        "            down_block_res_samples += res_samples\n",
        "\n",
        "        # 4. mid\n",
        "        sample = self.mid_block(\n",
        "            hidden_states=sample,\n",
        "        temb=emb,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            image_only_indicator=image_only_indicator,\n",
        "        )\n",
        "\n",
        "\n",
        "        # 5. Control net blocks\n",
        "\n",
        "        # initialize the controlnet_down_block_res_samples of it is on embpy nn.ModuleList\n",
        "        if self.controlnet_down_blocks is None:\n",
        "            self.controlnet_down_blocks = nn.ModuleList([])\n",
        "\n",
        "\n",
        "            for down_block_res_sample in down_block_res_samples:\n",
        "                # Determine the current number of channels in the tensor\n",
        "                current_channels = down_block_res_sample.size(1)\n",
        "\n",
        "                # Dynamically create a zero convolution block for the current tensor\n",
        "                controlnet_block = nn.Conv2d(current_channels, current_channels, kernel_size=1)\n",
        "                controlnet_block = zero_module(controlnet_block).to(down_block_res_sample.device, dtype=sample.dtype)\n",
        "\n",
        "\n",
        "                # Store the processed sample for further use\n",
        "                self.controlnet_down_blocks.append(controlnet_block)\n",
        "\n",
        "\n",
        "        controlnet_down_block_res_samples = ()\n",
        "\n",
        "        for index , (down_block_res_sample, controlnet_block) in enumerate(zip(down_block_res_samples, self.controlnet_down_blocks)):\n",
        "\n",
        "            # print to the debug console the device where the down_block_res_sample is\n",
        "            try:\n",
        "                # print the size of the down_block_res_sample\n",
        "                print(f\"Down block res sample shape before the conversion: {down_block_res_sample.shape}\")\n",
        "                down_block_res_sample = controlnet_block(down_block_res_sample)\n",
        "                controlnet_down_block_res_samples = controlnet_down_block_res_samples + (down_block_res_sample,)\n",
        "            except Exception as e:\n",
        "                # Print the error in conjecuntion with the index\n",
        "                print(f\"Error at index {index}: {e}\")\n",
        "\n",
        "        down_block_res_samples = controlnet_down_block_res_samples\n",
        "\n",
        "        mid_block_res_sample = self.controlnet_mid_block(sample)\n",
        "\n",
        "        down_block_res_samples = [sample for sample in down_block_res_samples]\n",
        "        mid_block_res_sample = mid_block_res_sample\n",
        "\n",
        "        if not return_dict:\n",
        "            return (down_block_res_samples, mid_block_res_sample)\n",
        "\n",
        "        return SpatioTemporalControlNetOutput(\n",
        "            down_block_res_samples=down_block_res_samples, mid_block_res_sample=mid_block_res_sample\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_unet(\n",
        "        cls,\n",
        "        unet: UNetSpatioTemporalConditionModel,\n",
        "        load_weights_from_unet: bool = True,\n",
        "    ):\n",
        "\n",
        "        addition_time_embed_dim = (\n",
        "            unet.config.addition_time_embed_dim if \"addition_time_embed_dim\" in unet.config else None\n",
        "        )\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # conditioning net\n",
        "        condition_net = CustomConditioningNet()\n",
        "        condition_net = condition_net.to(device)\n",
        "\n",
        "\n",
        "        controlnet = cls(\n",
        "            in_channels=unet.config.in_channels,\n",
        "            down_block_types=unet.config.down_block_types,\n",
        "            block_out_channels=unet.config.block_out_channels,  # What are block out channels\n",
        "            addition_time_embed_dim=addition_time_embed_dim,\n",
        "            projection_class_embeddings_input_dim=unet.config.projection_class_embeddings_input_dim,\n",
        "            layers_per_block=unet.config.layers_per_block,\n",
        "            cross_attention_dim=unet.config.cross_attention_dim,\n",
        "            transformer_layers_per_block=unet.config.transformer_layers_per_block,\n",
        "            num_attention_heads=unet.config.num_attention_heads,\n",
        "            conditioning_embedding = condition_net\n",
        "        )\n",
        "\n",
        "        if load_weights_from_unet:\n",
        "            controlnet.conv_in.load_state_dict(unet.conv_in.state_dict())\n",
        "            controlnet.time_proj.load_state_dict(unet.time_proj.state_dict())\n",
        "            controlnet.time_embedding.load_state_dict(unet.time_embedding.state_dict())\n",
        "\n",
        "            controlnet.down_blocks.load_state_dict(unet.down_blocks.state_dict())\n",
        "            controlnet.mid_block.load_state_dict(unet.mid_block.state_dict())\n",
        "\n",
        "        return controlnet\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        nn.init.zeros_(p)\n",
        "    return module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrmrKYD4O8tL",
        "outputId": "595cf5bd-7836-41e0-8f1c-09599d009a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 25, 320, 72, 128])\n"
          ]
        }
      ],
      "source": [
        "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
        "conditioning_net = CustomConditioningNet()\n",
        "hoi = conditioning_net.forward(pseudo_sample)\n",
        "print(hoi.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8wkjeg5O8tL",
        "outputId": "245a9d13-7205-4491-8979-e32207a64330"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpatioTemporalControlNet(\n",
              "  (conditioning_embedding): CustomConditioningNet(\n",
              "    (initial_conv): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (conv_blocks): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): SiLU()\n",
              "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): SiLU()\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): SiLU()\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): SiLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): SiLU()\n",
              "      )\n",
              "    )\n",
              "    (adaptive_pool): AdaptiveAvgPool2d(output_size=(72, 128))\n",
              "  )\n",
              "  (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (time_proj): Timesteps()\n",
              "  (time_embedding): TimestepEmbedding(\n",
              "    (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "  )\n",
              "  (add_time_proj): Timesteps()\n",
              "  (add_embedding): TimestepEmbedding(\n",
              "    (linear_1): LoRACompatibleLinear(in_features=768, out_features=1280, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "  )\n",
              "  (down_blocks): ModuleList(\n",
              "    (0): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
              "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=640, out_features=2560, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
              "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): CrossAttnDownBlockSpatioTemporal(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (temporal_transformer_blocks): ModuleList(\n",
              "            (0): TemporalBasicTransformerBlock(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (time_pos_embed): TimestepEmbedding(\n",
              "            (linear_1): LoRACompatibleLinear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): SiLU()\n",
              "            (linear_2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (time_proj): Timesteps()\n",
              "          (time_mixer): AlphaBlender()\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "        (1): SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): DownBlockSpatioTemporal(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x SpatioTemporalResBlock(\n",
              "          (spatial_res_block): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (temporal_res_block): TemporalResnetBlock(\n",
              "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "          (time_mixer): AlphaBlender()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (controlnet_mid_block): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (mid_block): UNetMidBlockSpatioTemporal(\n",
              "    (attentions): ModuleList(\n",
              "      (0): TransformerSpatioTemporalModel(\n",
              "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (transformer_blocks): ModuleList(\n",
              "          (0): BasicTransformerBlock(\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn1): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn2): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (temporal_transformer_blocks): ModuleList(\n",
              "          (0): TemporalBasicTransformerBlock(\n",
              "            (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff_in): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn1): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn2): Attention(\n",
              "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): LoRACompatibleLinear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (time_pos_embed): TimestepEmbedding(\n",
              "          (linear_1): LoRACompatibleLinear(in_features=1280, out_features=5120, bias=True)\n",
              "          (act): SiLU()\n",
              "          (linear_2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "        (time_proj): Timesteps()\n",
              "        (time_mixer): AlphaBlender()\n",
              "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (resnets): ModuleList(\n",
              "      (0-1): 2 x SpatioTemporalResBlock(\n",
              "        (spatial_res_block): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (temporal_res_block): TemporalResnetBlock(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (time_mixer): AlphaBlender()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# initialize the contrl net from my_net\n",
        "control_net = SpatioTemporalControlNet.from_unet(my_net)\n",
        "# control_net = control_net.half()\n",
        "control_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "006a38c4a8bc42ccb861390269e7c28e",
            "0eb5b9db30b74e7b92b113659c67ca18",
            "d45a2a94430d400d9b0cda86054d6b10",
            "7d9385afbe2b4277afb72fe9038cdae8",
            "4842b2b261c14c2bbfec48b7e633f1a1",
            "6df056729ff744498718d6d6dc120cce",
            "14c70dbe95bc49559f1c02fd651bba9e",
            "9773cbbaf0cb49b99b43f27080fcd569",
            "4617d70cc9d24f58a621839d0ee1e1b9",
            "b4ae0e7018794621825189108ffb1da8",
            "d6d0e39a42cb4a8b82b8023cbce2ec83"
          ]
        },
        "id": "0CcKELIXO8tL",
        "outputId": "bdfe31c0-dce4-49e0-f4a2-4656e28cd2db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "006a38c4a8bc42ccb861390269e7c28e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
        "\n",
        "tokenizer = pipeline.tokenizer\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMVDs8kbO8tL",
        "outputId": "ff35f8de-55d7-45e8-8a40-bbaf55b1b8ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import gc\n",
        "tokenizer = pipeline.tokenizer\n",
        "text_encoder = pipeline.text_encoder\n",
        "del pipeline\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65awGw3vO8tL",
        "outputId": "6db6f20e-c5ee-406f-f2a9-958106496e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
            "torch.Size([2, 1, 1024])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
        "from typing import Optional, List\n",
        "\n",
        "def encode_prompt(\n",
        "    prompt,\n",
        "    device,\n",
        "    do_classifier_free_guidance,\n",
        "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
        "    num_images_per_prompt = 1,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    lora_scale: Optional[float] = None,\n",
        "    clip_skip: Optional[int] = None,\n",
        "    text_encoder = None,\n",
        "    tokenizer = None):\n",
        "    # Set the text_encoder and the tokenizer on the correct device\n",
        "    text_encoder = text_encoder.to(device)\n",
        "\n",
        "\n",
        "    if prompt is not None and isinstance(prompt, str):\n",
        "        batch_size = 1\n",
        "    elif prompt is not None and isinstance(prompt, list):\n",
        "        batch_size = len(prompt)\n",
        "    else:\n",
        "        batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "    if True:\n",
        "\n",
        "        text_inputs = tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_input_ids = text_inputs.input_ids\n",
        "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
        "            text_input_ids, untruncated_ids\n",
        "        ):\n",
        "            removed_text = tokenizer.batch_decode(\n",
        "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
        "            )\n",
        "            logger.warning(\n",
        "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
        "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
        "            )\n",
        "\n",
        "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
        "            attention_mask = text_inputs.attention_mask.to(device)\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "\n",
        "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
        "        prompt_embeds = prompt_embeds[0]\n",
        "\n",
        "\n",
        "    if text_encoder is not None:\n",
        "        prompt_embeds_dtype = text_encoder.dtype\n",
        "\n",
        "\n",
        "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
        "\n",
        "\n",
        "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
        "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
        "\n",
        "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
        "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "    # get unconditional embeddings for classifier free guidance\n",
        "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
        "        uncond_tokens: List[str]\n",
        "        if negative_prompt is None:\n",
        "            uncond_tokens = [\"\"] * batch_size\n",
        "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
        "            raise TypeError(\n",
        "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
        "                f\" {type(prompt)}.\"\n",
        "            )\n",
        "        elif isinstance(negative_prompt, str):\n",
        "            uncond_tokens = [negative_prompt]\n",
        "        elif batch_size != len(negative_prompt):\n",
        "            raise ValueError(\n",
        "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
        "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
        "                \" the batch size of `prompt`.\"\n",
        "            )\n",
        "        else:\n",
        "            uncond_tokens = negative_prompt\n",
        "\n",
        "        max_length = prompt_embeds.shape[1]\n",
        "        uncond_input = tokenizer(\n",
        "            uncond_tokens,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
        "            attention_mask = uncond_input.attention_mask.to(device)\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        negative_prompt_embeds = text_encoder(\n",
        "            uncond_input.input_ids.to(device),\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
        "\n",
        "    if do_classifier_free_guidance:\n",
        "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
        "        seq_len = negative_prompt_embeds.shape[1]\n",
        "\n",
        "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
        "\n",
        "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
        "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "\n",
        "\n",
        "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
        "    embeds = embeds.mean(dim=1, keepdim=True)\n",
        "\n",
        "    return embeds\n",
        "\n",
        "prompt = \"driving scene\"\n",
        "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
        "print(prompt_embeds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ec6LVoO8tL",
        "outputId": "648fb461-8a5e-49ae-8bc0-973935c578ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2/snapshots/1e128c8891e52218b74cde8f26dbfc701cb99d79/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb5bENKQO8tL",
        "outputId": "abcc4225-12ce-44de-c804-22afbc14442c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 1024])\n"
          ]
        }
      ],
      "source": [
        "hoi = torch.zeros(1, 1, 1024).to(device)\n",
        "print(hoi.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdEL8SgoO8tM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "po4NBCuEO8tM",
        "outputId": "efa839ea-78c1-4ed3-a068-5b77748fc7ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'latent_model_input' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e4776f5081ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# print on which device the input is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Latent model input is on: {latent_model_input.device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hidden image embeddings are on: {hidden_image_embeddings.device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Added time IDs are on: {added_time_ids.device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'latent_model_input' is not defined"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    time =  torch.tensor(1).to(dtype=dtype)\n",
        "\n",
        "    # print on which device the input is\n",
        "    print(f\"Latent model input is on: {latent_model_input.device}\")\n",
        "    print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
        "    print(f\"Added time IDs are on: {added_time_ids.device}\")\n",
        "    print(f\"Time is on: {time.device}\")\n",
        "\n",
        "    # move time to divice\n",
        "    time = time.to(device)\n",
        "    print(f\"Time is on: {time.device}\")\n",
        "\n",
        "    noise_pred = control_net.forward(\n",
        "        torch.ones(2, 25, 8, 72, 128).to(dtype=dtype, device=device),\n",
        "        time,\n",
        "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
        "        encoder_hidden_states = prompt_embeds.to(dtype=dtype),\n",
        "        # encoder_hidden_states = None,\n",
        "        return_dict=True,\n",
        "        controlnet_condition = torch.ones(25, 4, 576, 1024).to(dtype=dtype, device=device)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Print the sizes of the tensors\n",
        "\n",
        "    if noise_pred is not None:\n",
        "        del noise_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Oj1mgFO8tM",
        "outputId": "31e8929e-ce73-46e9-820f-76ae95518efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of encoder hidden states with: torch.Size([2, 1, 1024])\n",
            "Sample shape before the conversion: torch.Size([2, 8, 64, 64])\n",
            "Sample shape after the conversion: torch.Size([2, 320, 64, 64])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 320, 32, 32])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 640, 32, 32])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 640, 32, 32])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 640, 16, 16])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 1280, 16, 16])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 1280, 16, 16])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
            "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
            "Length of down_block_res_samples afterprocesssing: 12\n",
            "This is the batch size 2\n",
            "This is the iteration 0\n",
            "This is the iteration 1\n",
            "This is the iteration 2\n",
            "This is the iteration 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    (down_block_res_samples, mid_block_res_samples) = control_net.forward(\n",
        "        latent_model_input.to(dtype=dtype),\n",
        "        time,\n",
        "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
        "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "    # reverse the down_block_res_samples tuple\n",
        "    # down_block_res_samples = down_block_res_samples[::-1]\n",
        "\n",
        "    print(f\"Length of down_block_res_samples afterprocesssing: {len(down_block_res_samples)}\")\n",
        "\n",
        "\n",
        "    noise_pred = my_net.forward(\n",
        "        latent_model_input.to(dtype=dtype),\n",
        "        torch.tensor(1).to(dtype=dtype, device=device),\n",
        "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
        "        # Maybe I need to reverse the order of the tensors\n",
        "        down_block_additional_residuals= down_block_res_samples,\n",
        "        mid_block_additional_residual = mid_block_res_samples,\n",
        "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
        "        return_dict=False,\n",
        "    )[0]\n",
        "\n",
        "    # Print the sizes of the tensors\n",
        "    if noise_pred is not None:\n",
        "        del noise_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0TXqRXHCO8tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image = load_image(\"/content/frame0.png\")\n",
        "\n",
        "pseudo_sample = torch.zeros(25, 4, 578, 1028).to(device)"
      ],
      "metadata": {
        "id": "4N0bp3z2SIQv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "arYfyDEEO8tM"
      },
      "outputs": [],
      "source": [
        "pipe_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
        "    vae = pipe.vae,\n",
        "    image_encoder = pipe.image_encoder,\n",
        "    unet=my_net,\n",
        "    scheduler=pipe.scheduler,\n",
        "    feature_extractor=pipe.feature_extractor,\n",
        "    controlnet=control_net,\n",
        "    tokenizer = tokenizer,\n",
        "    text_encoder = text_encoder\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del my_net\n",
        "# del control_net\n",
        "# del pseudo_sample\n",
        "# del conditioning_net"
      ],
      "metadata": {
        "id": "jFpVljSPMKkj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW2RqRBnMgGB",
        "outputId": "975b8786-69e0-4f8e-d34a-b8e6ef64aff1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = load_image(\"/content/frame0.png\")"
      ],
      "metadata": {
        "id": "aqlo7vEqaiuY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odlwVINiamx_",
        "outputId": "5d988dd3-3290-4c7e-a18e-d292a753d581"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=640x360 at 0x7C0DD9D1F3D0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"dom\"\n",
        "pseudo_image = torch.zeros(3,576,1024).to(device = device)\n",
        "frames = pipe_with_controlnet(image = image, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n",
        "\n",
        "export_to_video(frames, \"car_scene.mp4\", fps=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651,
          "referenced_widgets": [
            "d2fd1c00d3d44aab8e251eb36b15b6ec",
            "21885c8c17064a0185c1618b973851c3",
            "d07ecd5386a04ef588af655dc4b19e6b",
            "3da067173b034bf792a9cf71366afaac",
            "52f75d440d7b415996ae02743b6ca899",
            "02a0b647614144eba8e38ee5e2f87266",
            "789e955174a34add967a1e48226faec7",
            "60736212efe84ec98d65c74e8390644c",
            "c349b5cc83f14074917a7cd7720f4c0c",
            "8203a1a5c35143149d531c5cfb8d8b6a",
            "59bad37185784a448016f89db2ba32f3"
          ]
        },
        "id": "P_2siTRkMHgi",
        "outputId": "94f062a5-1e41-4e4a-a615-e029a1e57d70"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2fd1c00d3d44aab8e251eb36b15b6ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of encoder hidden states with: torch.Size([50, 1, 1024])\n",
            "Sample shape before the conversion: torch.Size([50, 8, 72, 128])\n",
            "Sample shape after the conversion: torch.Size([50, 320, 72, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0ee110795df7>:347: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
            "  controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 82.38 MiB is free. Process 186318 has 15.69 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 811.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1a38c5f9fe3c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dom\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpseudo_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m576\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_with_controlnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudo_sample\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdecode_chunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexport_to_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"car_scene.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion_with_controlnet.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image, prompt, conditioning_image, height, width, num_frames, num_inference_steps, min_guidance_scale, max_guidance_scale, fps, motion_bucket_id, noise_aug_strength, decode_chunk_size, num_videos_per_prompt, generator, latents, output_type, callback_on_step_end, callback_on_step_end_tensor_inputs, return_dict)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mlatent_model_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_latents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 down_block_res_samples, mid_block_res_sample = self.controlnet.forward(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-0ee110795df7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, added_time_ids, return_dict, controlnet_condition)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdownsample_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdownsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 sample, res_samples = downsample_block(\n\u001b[0m\u001b[1;32m    364\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/unets/unet_3d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, image_only_indicator)\u001b[0m\n\u001b[1;32m   2171\u001b[0m                     \u001b[0mimage_only_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_only_indicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m                 )\n\u001b[0;32m-> 2173\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m   2174\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/transformer_temporal.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, image_only_indicator, return_dict)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 )\n\u001b[1;32m    350\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mgligen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gligen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         attn_output = self.attn1(\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_cross_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# here we simply pass along all tensors to the selected processor class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         return self.processor(\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, scale)\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_encoder_hidden_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_diffusers_library/src/diffusers/models/lora.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 82.38 MiB is free. Process 186318 has 15.69 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 811.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e8a68c410ded4350a845be8b8a5b615e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c270ca0f5d3049a1ab485e7989548631",
              "IPY_MODEL_73ccace5c44d4a6dacd6a61c5c2b5fb6",
              "IPY_MODEL_623a3f6716e0470c9eab6cf7324fe464"
            ],
            "layout": "IPY_MODEL_9d1b1fb54ce04956a8d099ff79e158ae"
          }
        },
        "c270ca0f5d3049a1ab485e7989548631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a68d1ac3d7644cc97ae1550f0c43414",
            "placeholder": "​",
            "style": "IPY_MODEL_380df1a25b8e46b6b2c0e0b638692362",
            "value": ""
          }
        },
        "73ccace5c44d4a6dacd6a61c5c2b5fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76411395febb4cf2bb668bb05c7555d9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da548536353640e39fe84966f56a1b4d",
            "value": 0
          }
        },
        "623a3f6716e0470c9eab6cf7324fe464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebbd3b535a10443badb4e029524c7997",
            "placeholder": "​",
            "style": "IPY_MODEL_b431df09e6334fe0897dc321dfe82f78",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "9d1b1fb54ce04956a8d099ff79e158ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a68d1ac3d7644cc97ae1550f0c43414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "380df1a25b8e46b6b2c0e0b638692362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76411395febb4cf2bb668bb05c7555d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "da548536353640e39fe84966f56a1b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebbd3b535a10443badb4e029524c7997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b431df09e6334fe0897dc321dfe82f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38a4596231e444bb6c677a8653d0b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b3d3740c5114152910dbd3a5b5f3e23",
              "IPY_MODEL_582f087b56fe4f88bbed0b396d9a7248",
              "IPY_MODEL_90bea31d332f45818ae0c32c654b46e8"
            ],
            "layout": "IPY_MODEL_d730b85a76a947a087f9293dfa02fde4"
          }
        },
        "3b3d3740c5114152910dbd3a5b5f3e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34516a7f5304436caba0906477c4ec5e",
            "placeholder": "​",
            "style": "IPY_MODEL_553d5482a74b4aea9b62e0b0277126e8",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "582f087b56fe4f88bbed0b396d9a7248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45731d5bccdb4317a6deabc5686b712b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8adc2843531b41eeb4d5be3158cbe230",
            "value": 5
          }
        },
        "90bea31d332f45818ae0c32c654b46e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c07b19e3be41ea89ce2228c25ef88d",
            "placeholder": "​",
            "style": "IPY_MODEL_f980c39e3d1e44fbb84d56859eeb8964",
            "value": " 5/5 [00:01&lt;00:00, 20.01it/s]"
          }
        },
        "d730b85a76a947a087f9293dfa02fde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34516a7f5304436caba0906477c4ec5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553d5482a74b4aea9b62e0b0277126e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45731d5bccdb4317a6deabc5686b712b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8adc2843531b41eeb4d5be3158cbe230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17c07b19e3be41ea89ce2228c25ef88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f980c39e3d1e44fbb84d56859eeb8964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "006a38c4a8bc42ccb861390269e7c28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eb5b9db30b74e7b92b113659c67ca18",
              "IPY_MODEL_d45a2a94430d400d9b0cda86054d6b10",
              "IPY_MODEL_7d9385afbe2b4277afb72fe9038cdae8"
            ],
            "layout": "IPY_MODEL_4842b2b261c14c2bbfec48b7e633f1a1"
          }
        },
        "0eb5b9db30b74e7b92b113659c67ca18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df056729ff744498718d6d6dc120cce",
            "placeholder": "​",
            "style": "IPY_MODEL_14c70dbe95bc49559f1c02fd651bba9e",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "d45a2a94430d400d9b0cda86054d6b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9773cbbaf0cb49b99b43f27080fcd569",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4617d70cc9d24f58a621839d0ee1e1b9",
            "value": 6
          }
        },
        "7d9385afbe2b4277afb72fe9038cdae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ae0e7018794621825189108ffb1da8",
            "placeholder": "​",
            "style": "IPY_MODEL_d6d0e39a42cb4a8b82b8023cbce2ec83",
            "value": " 6/6 [00:01&lt;00:00,  2.96it/s]"
          }
        },
        "4842b2b261c14c2bbfec48b7e633f1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df056729ff744498718d6d6dc120cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c70dbe95bc49559f1c02fd651bba9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9773cbbaf0cb49b99b43f27080fcd569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4617d70cc9d24f58a621839d0ee1e1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4ae0e7018794621825189108ffb1da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d0e39a42cb4a8b82b8023cbce2ec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2fd1c00d3d44aab8e251eb36b15b6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21885c8c17064a0185c1618b973851c3",
              "IPY_MODEL_d07ecd5386a04ef588af655dc4b19e6b",
              "IPY_MODEL_3da067173b034bf792a9cf71366afaac"
            ],
            "layout": "IPY_MODEL_52f75d440d7b415996ae02743b6ca899"
          }
        },
        "21885c8c17064a0185c1618b973851c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02a0b647614144eba8e38ee5e2f87266",
            "placeholder": "​",
            "style": "IPY_MODEL_789e955174a34add967a1e48226faec7",
            "value": "  0%"
          }
        },
        "d07ecd5386a04ef588af655dc4b19e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60736212efe84ec98d65c74e8390644c",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c349b5cc83f14074917a7cd7720f4c0c",
            "value": 0
          }
        },
        "3da067173b034bf792a9cf71366afaac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8203a1a5c35143149d531c5cfb8d8b6a",
            "placeholder": "​",
            "style": "IPY_MODEL_59bad37185784a448016f89db2ba32f3",
            "value": " 0/25 [00:00&lt;?, ?it/s]"
          }
        },
        "52f75d440d7b415996ae02743b6ca899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02a0b647614144eba8e38ee5e2f87266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789e955174a34add967a1e48226faec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60736212efe84ec98d65c74e8390644c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c349b5cc83f14074917a7cd7720f4c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8203a1a5c35143149d531c5cfb8d8b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bad37185784a448016f89db2ba32f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}