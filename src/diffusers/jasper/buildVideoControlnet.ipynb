{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "\n",
    "\n",
    "from types import MethodType\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'num_frames': 2} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\", num_frames = 2\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlnet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict([('vae', ('diffusers', 'AutoencoderKLTemporalDecoder')), ('image_encoder', ('transformers', 'CLIPVisionModelWithProjection')), ('unet', ('diffusers', 'UNetSpatioTemporalConditionModel')), ('scheduler', ('diffusers', 'EulerDiscreteScheduler')), ('feature_extractor', ('transformers', 'CLIPImageProcessor')), ('_name_or_path', 'stabilityai/stable-video-diffusion-img2vid-xt')])\n"
     ]
    }
   ],
   "source": [
    "# initialize the contrl net from my_net\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "del pipeline\n",
    "gc.collect() \n",
    "\n",
    "pipe_config = pipe.config\n",
    "print(pipe_config)\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "my_net = UNetSpatioTemporalConditionModel()\n",
    "my_net.load_state_dict(unet_weights)\n",
    "control_net = SpatioTemporalControlNet.from_unet(my_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    unet=my_net,\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    controlnet=control_net,\n",
    "    tokenizer = tokenizer,\n",
    "    text_encoder = text_encoder\n",
    ")\n",
    "pipe_with_controlnet.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/tmp/ipykernel_14713/107138406.py:373: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  if next(self.conditioning_embedding.parameters()).is_cuda:\n",
      "/tmp/ipykernel_14713/107138406.py:374: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  conditioning_nn = next(self.conditioning_embedding.parameters()).device\n",
      "/tmp/ipykernel_14713/107138406.py:378: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  conditioning_nn_dtype = next(self.conditioning_embedding.parameters()).dtype\n",
      "/tmp/ipykernel_14713/107138406.py:389: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n",
      "100%|██████████| 25/25 [01:34<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"d\"\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "# Define a simple torch generator\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "image = load_image(\"/home/wisley/custom_diffusers_library/frame2.png\")\n",
    "frames = pipe_with_controlnet(image = image, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajavi.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(frames, \"jajavi.mp4\", fps=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input dtype: torch.float16\n",
      "Hidden image embeddings dtype: torch.float16\n",
      "torch.Size([2, 1, 8, 64, 64])\n",
      "torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 3])\n",
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def prepare_latents(\n",
    "    batch_size,\n",
    "    num_frames,\n",
    "    num_channels_latents,\n",
    "    height,\n",
    "    width,\n",
    "    dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents=None,\n",
    "):\n",
    "    shape = (\n",
    "        batch_size,\n",
    "        num_frames,\n",
    "        num_channels_latents // 2,\n",
    "        height // 1,\n",
    "        width // 1,\n",
    "    )\n",
    "    if isinstance(generator, list) and len(generator) != batch_size:\n",
    "        raise ValueError(\n",
    "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "        )\n",
    "\n",
    "    if latents is None:\n",
    "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "    else:\n",
    "        latents = latents.to(device)\n",
    "\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * 0.2\n",
    "    return latents\n",
    "\n",
    "def pseudo_image_embeddings( shape, generator, device, dtype, do_classifier_free_guidance = True ):\n",
    "    image_embeddings = randn_tensor(shape, generator=generator, device= device, dtype=dtype)\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        return torch.cat([negative_image_embeddings, image_embeddings])\n",
    "\n",
    "def get_add_time_ids(\n",
    "  fps = 7,\n",
    "  motion_bucket_id = 127,\n",
    "  noise_aug_strength = 0.02,\n",
    "  dtype = torch.float32,\n",
    "  batch_size = 1,\n",
    "  num_videos_per_prompt = 1,\n",
    "  do_classifier_free_guidance = True,\n",
    "):\n",
    "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "  if do_classifier_free_guidance:\n",
    "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "  return add_time_ids\n",
    "\n",
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Generate original latents with specified dtype\n",
    "my_latents = prepare_latents(1, 1, 8, 64, 64, dtype, device, generator)\n",
    "\n",
    "# Apply classifier-free guidance by duplicating the latents and ensuring the correct dtype\n",
    "latent_model_input = torch.cat([my_latents] * 2)  # Inherits dtype from my_latents\n",
    "\n",
    "# Create pseudo image latents by cloning the original latents\n",
    "pseudo_image_latents = latent_model_input.clone()  # Inherits dtype\n",
    "\n",
    "# Concatenate pseudo image latents over the channels dimension, ensuring matching dtype\n",
    "latent_model_input = torch.cat([latent_model_input, pseudo_image_latents], dim=2)\n",
    "\n",
    "\n",
    "# Create the fake image embeddings with the specified dtype\n",
    "hidden_image_embeddings = pseudo_image_embeddings((1, 1, 1024), generator, device, dtype)\n",
    "\n",
    "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
    "\n",
    "# Verify the dtype of both tensors\n",
    "print(f\"Latent model input dtype: {latent_model_input.dtype}\")\n",
    "print(f\"Hidden image embeddings dtype: {hidden_image_embeddings.dtype}\")\n",
    "\n",
    "print(latent_model_input.shape)\n",
    "print(hidden_image_embeddings.shape)\n",
    "print(added_time_ids.shape)\n",
    "\n",
    "# Print on which model they are\n",
    "print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "# Assuming added_time_ids is also a tensor; replace this with the actual tensor variable if different\n",
    "print(f\"Added time IDs are on: {added_time_ids.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        down_block_additional_residuals= None,\n",
    "        mid_block_additional_residual = None,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    print(noise_pred.shape)\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    do_classifier_free_guidance,\n",
    "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
    "    num_images_per_prompt = 1,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    text_encoder = None, \n",
    "    tokenizer = None):\n",
    "    # Set the text_encoder and the tokenizer on the correct device  \n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if True:\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "            text_input_ids, untruncated_ids\n",
    "        ):\n",
    "            removed_text = tokenizer.batch_decode(\n",
    "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "            )\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = text_inputs.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "       \n",
    "\n",
    "    if text_encoder is not None:\n",
    "        prompt_embeds_dtype = text_encoder.dtype\n",
    "\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
    "    \n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        uncond_tokens: List[str]\n",
    "        if negative_prompt is None:\n",
    "            uncond_tokens = [\"\"] * batch_size\n",
    "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "            raise TypeError(\n",
    "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                f\" {type(prompt)}.\"\n",
    "            )\n",
    "        elif isinstance(negative_prompt, str):\n",
    "            uncond_tokens = [negative_prompt]\n",
    "        elif batch_size != len(negative_prompt):\n",
    "            raise ValueError(\n",
    "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                \" the batch size of `prompt`.\"\n",
    "            )\n",
    "        else:\n",
    "            uncond_tokens = negative_prompt\n",
    "        \n",
    "        max_length = prompt_embeds.shape[1]\n",
    "        uncond_input = tokenizer(\n",
    "            uncond_tokens,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = uncond_input.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        negative_prompt_embeds = text_encoder(\n",
    "            uncond_input.input_ids.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "\n",
    "    \n",
    "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
    "    embeds = embeds.mean(dim=1, keepdim=True)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "prompt = \"A f a cat\"\n",
    "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "print(prompt_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n",
      "Time is on: cpu\n",
      "Time is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    time =  torch.tensor(1).to(dtype=dtype)\n",
    "\n",
    "    # print on which device the input is\n",
    "    print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "    print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "    print(f\"Added time IDs are on: {added_time_ids.device}\")\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "\n",
    "    # move time to divice\n",
    "    time = time.to(device)\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "           \n",
    "    noise_pred = control_net.forward(\n",
    "        torch.ones(2, 2, 8, 64, 64).to(dtype=dtype, device=device),\n",
    "        time,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        encoder_hidden_states = prompt_embeds.to(dtype=dtype),\n",
    "        # encoder_hidden_states = None,\n",
    "        return_dict=True,\n",
    "        # controlnet_condition = torch.ones(25, 4, 576, 1024).to(dtype=dtype, device=device)\n",
    "    )    \n",
    "    \n",
    "\n",
    "    # Print the sizes of the tensors\n",
    " \n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of down_block_res_samples afterprocesssing: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    (down_block_res_samples, mid_block_res_samples) = control_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        time,\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    # reverse the down_block_res_samples tuple\n",
    "    # down_block_res_samples = down_block_res_samples[::-1]\n",
    "\n",
    "    print(f\"Length of down_block_res_samples afterprocesssing: {len(down_block_res_samples)}\")\n",
    "\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        # Maybe I need to reverse the order of the tensors\n",
    "        down_block_additional_residuals= down_block_res_samples,\n",
    "        mid_block_additional_residual = mid_block_res_samples,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    # Print the sizes of the tensors\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 320, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "conditioning_net = CustomConditioningNet()\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "hoi = conditioning_net.forward(pseudo_sample)\n",
    "print(hoi.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edit_diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
