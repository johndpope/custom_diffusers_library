{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "\n",
    "\n",
    "from types import MethodType\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'num_frames': 2} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  7.20it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\", num_frames = 2\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlnet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConditioningNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        # Initial convolution to match the first target channel dimension\n",
    "        self.initial_conv = nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Defining a series of convolutional blocks to progressively downsample\n",
    "        # and increase channel dimensions towards the target size\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 320, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Final adjustment to target spatial dimensions\n",
    "        # Considering a final adaptive pooling layer to ensure matching to the target spatial size (64x64)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((72, 128))\n",
    "\n",
    "\n",
    "    def cast_model_to(self, device, dtype):\n",
    "        \"\"\"\n",
    "        Casts all parameters and buffers of a given model to the specified device and data type.\n",
    "\n",
    "        Parameters:\n",
    "        - model: An instance of torch.nn.Module whose parameters and buffers are to be cast.\n",
    "        - device: The target device (e.g., 'cuda:0', 'cpu').\n",
    "        - dtype: The target data type (e.g., torch.float32, torch.float16).\n",
    "\n",
    "        Returns:\n",
    "        - The model with its parameters and buffers cast to the specified device and data type.\n",
    "        \"\"\"\n",
    "        # Cast all parameters to the specified device and dtype.\n",
    "        for param in self.parameters():\n",
    "            param.data = param.data.to(device=device, dtype=dtype)\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad.data = param.grad.data.to(device=device, dtype=dtype)\n",
    "\n",
    "        # Cast all buffers (non-learnable parameters, e.g., running mean in BatchNorm) to the specified device and dtype.\n",
    "        for buffer in self.buffers():\n",
    "            buffer.data = buffer.data.to(device=device, dtype=ddtype)\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = torch.cat([x,x]) \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class UNetSpatioTemporalConditionOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    The output of [`UNetSpatioTemporalConditionModel`].\n",
    "\n",
    "    Args:\n",
    "        sample (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
    "            The hidden states output conditioned on `encoder_hidden_states` input. Output of last layer of model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sample: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class SpatioTemporalControlNetOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    The output of [`ControlNetModel`].\n",
    "\n",
    "    Args:\n",
    "        down_block_res_samples (`tuple[torch.Tensor]`):\n",
    "            A tuple of downsample activations at different resolutions for each downsampling block. Each tensor should\n",
    "            be of shape `(batch_size, channel * resolution, height //resolution, width // resolution)`. Output can be\n",
    "            used to condition the original UNet's downsampling activations.\n",
    "        mid_down_block_re_sample (`torch.Tensor`):\n",
    "            The activation of the midde block (the lowest sample resolution). Each tensor should be of shape\n",
    "            `(batch_size, channel * lowest_resolution, height // lowest_resolution, width // lowest_resolution)`.\n",
    "            Output can be used to condition the original UNet's middle block activation.\n",
    "    \"\"\"\n",
    "\n",
    "    down_block_res_samples: Tuple[torch.Tensor]\n",
    "    mid_block_res_sample: torch.Tensor\n",
    "    \n",
    "    # Add a class which prints the sizes of the tensors\n",
    "    def print_sizes(self):\n",
    "        print(f\"Down block res samples: {self.down_block_res_samples[0].shape}\")\n",
    "        print(f\"Mid block res sample: {self.mid_block_res_sample.shape}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SpatioTemporalControlNet(ModelMixin, ConfigMixin):\n",
    "    \"\"\"\n",
    "    A SpatioTemporalControlNet model for conditioning on spatio-temporal data.\n",
    "    This model adapts concepts from both ControlNetModel and UNetSpatioTemporalConditionModel,\n",
    "    focusing on handling video frames over time.\n",
    "    \"\"\"\n",
    "\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: Optional[int] = None,\n",
    "        in_channels: int = 8,\n",
    "        down_block_types: Tuple[str] = (\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"DownBlockSpatioTemporal\",\n",
    "        ),\n",
    "        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n",
    "        addition_time_embed_dim: int = 256,\n",
    "        projection_class_embeddings_input_dim: int = 768,\n",
    "        layers_per_block: Union[int, Tuple[int]] = 2,\n",
    "        cross_attention_dim: Union[int, Tuple[int]] = 1024,\n",
    "        transformer_layers_per_block: Union[int, Tuple[int], Tuple[Tuple]] = 1,\n",
    "        num_attention_heads: Union[int, Tuple[int]] = (5, 10, 10, 20),\n",
    "        conditioning_embedding = None\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.conditioning_embedding = conditioning_embedding\n",
    "\n",
    "        # input\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels,\n",
    "            block_out_channels[0],\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # time\n",
    "        time_embed_dim = block_out_channels[0] * 4\n",
    "\n",
    "        self.time_proj = Timesteps(block_out_channels[0], True, downscale_freq_shift=0)\n",
    "        timestep_input_dim = block_out_channels[0]\n",
    "\n",
    "        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "\n",
    "        self.add_time_proj = Timesteps(addition_time_embed_dim, True, downscale_freq_shift=0)\n",
    "        self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)\n",
    "\n",
    "\n",
    "        output_channel = block_out_channels[0]\n",
    "        \n",
    "        self.controlnet_down_blocks = None\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "        \n",
    "        if isinstance(num_attention_heads, int):\n",
    "            num_attention_heads = (num_attention_heads,) * len(down_block_types)\n",
    "\n",
    "        if isinstance(cross_attention_dim, int):\n",
    "            cross_attention_dim = (cross_attention_dim,) * len(down_block_types)\n",
    "\n",
    "        if isinstance(layers_per_block, int):\n",
    "            layers_per_block = [layers_per_block] * len(down_block_types)\n",
    "\n",
    "        if isinstance(transformer_layers_per_block, int):\n",
    "            transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n",
    "\n",
    "        blocks_time_embed_dim = time_embed_dim\n",
    "\n",
    "        # Initialize the connection between the down blocks and the unet\n",
    "        output_channel = block_out_channels[0]\n",
    "\n",
    "        # controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "        # controlnet_block = zero_module(controlnet_block)\n",
    "        # self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "        # down\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                temb_channels=blocks_time_embed_dim,\n",
    "                num_layers=layers_per_block[i],\n",
    "                transformer_layers_per_block=transformer_layers_per_block[i],\n",
    "                add_downsample= not is_final_block,\n",
    "                resnet_eps=1e-5,\n",
    "                down_block_type=down_block_type,\n",
    "                cross_attention_dim=cross_attention_dim[i],\n",
    "                num_attention_heads=num_attention_heads[i],\n",
    "                resnet_act_fn=\"silu\",\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "            # for _ in range(layers_per_block[i]):\n",
    "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "            #     controlnet_block = zero_module(controlnet_block)\n",
    "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "            # if not is_final_block:\n",
    "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "            #     controlnet_block = zero_module(controlnet_block)\n",
    "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # hardcoded_controlnet_block_dims = [320,320, 640,640, 1280, 1280, 1280,1280,1280]\n",
    "        # for index, controlnet_block_dim in enumerate(hardcoded_controlnet_block_dims):\n",
    "        #     controlnet_block = nn.Conv2d(controlnet_block_dim, controlnet_block_dim, kernel_size=1)\n",
    "        #     controlnet_block = zero_module(controlnet_block)\n",
    "        #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "\n",
    "        # Connections for the mid block\n",
    "        mid_block_channel = block_out_channels[-1]\n",
    "\n",
    "        controlnet_block = nn.Conv2d(mid_block_channel, mid_block_channel, kernel_size=1)\n",
    "        controlnet_block = zero_module(controlnet_block)\n",
    "        self.controlnet_mid_block = controlnet_block\n",
    "\n",
    "\n",
    "        # mid\n",
    "        self.mid_block = UNetMidBlockSpatioTemporal(\n",
    "            block_out_channels[-1],\n",
    "            temb_channels=blocks_time_embed_dim,\n",
    "            transformer_layers_per_block=transformer_layers_per_block[-1],\n",
    "            cross_attention_dim=cross_attention_dim[-1],\n",
    "            num_attention_heads=num_attention_heads[-1],\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        added_time_ids: torch.Tensor,\n",
    "        return_dict: bool = True,\n",
    "        controlnet_condition : torch.FloatTensor = None,\n",
    "    ) -> Union[UNetSpatioTemporalConditionOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNetSpatioTemporalConditionModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`):\n",
    "                The noisy input tensor with the following shape `(batch, num_frames, channel, height, width)`.\n",
    "            timestep (`torch.FloatTensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                The encoder hidden states with shape `(batch, sequence_length, cross_attention_dim)`.\n",
    "            added_time_ids: (`torch.FloatTensor`):\n",
    "                The additional time ids with shape `(batch, num_additional_ids)`. These are encoded with sinusoidal\n",
    "                embeddings and added to the time embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] instead of a plain\n",
    "                tuple.\n",
    "        Returns:\n",
    "            [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] is returned, otherwise\n",
    "                a `tuple` is returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size, num_frames = sample.shape[:2]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        time_embeds = self.add_time_proj(added_time_ids.flatten())\n",
    "        time_embeds = time_embeds.reshape((batch_size, -1))\n",
    "        time_embeds = time_embeds.to(emb.dtype)\n",
    "        aug_emb = self.add_embedding(time_embeds)\n",
    "        emb = emb + aug_emb\n",
    "\n",
    "        # Flatten the batch and frames dimensions\n",
    "        # sample: [batch, frames, channels, height, width] -> [batch * frames, channels, height, width]\n",
    "        sample = sample.flatten(0, 1)\n",
    "\n",
    "        \n",
    "        # Repeat the embeddings num_video_frames times\n",
    "        # emb: [batch, channels] -> [batch * frames, channels]\n",
    "        emb = emb.repeat_interleave(num_frames, dim=0).to(sample.device)\n",
    "        # encoder_hidden_states: [batch, 1, channels] -> [batch * frames, 1, channels]\n",
    "        # Let encoder_hidden_states be just zeros in the correct format\n",
    "        # shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
    "\n",
    "        \n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
    "            encoder_hidden_states = torch.zeros(shape_encoder_hidden_states, device=sample.device).repeat_interleave(num_frames, dim=0).to(dtype=sample.dtype)\n",
    "            # print(f\"Shape of encoder hidden states without: {encoder_hidden_states.shape}\")\n",
    "        else: \n",
    "            encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=0)\n",
    "            # print(f\"Shape of encoder hidden states with: {encoder_hidden_states.shape}\")\n",
    "        \n",
    "        # Print the shape of the sample\n",
    "        # 2. pre-process\n",
    "        # print(f\"Sample shape before the conversion: {sample.shape}\")\n",
    "        sample = self.conv_in(sample)\n",
    "        # print(f\"Sample shape after the conversion: {sample.shape}\")\n",
    "\n",
    "\n",
    "        # Make sure the controlnet_condition model and the controlet have if same type\n",
    "        # And are running on the same device\n",
    "\n",
    "        if controlnet_condition is not None: \n",
    "\n",
    "            current_device = sample.device\n",
    "            current_dtype = sample.dtype \n",
    "\n",
    "            if next(self.conditioning_embedding.parameters()).is_cuda:\n",
    "                conditioning_nn = next(self.conditioning_embedding.parameters()).device\n",
    "            else:\n",
    "                conditioning_nn = torch.device('cpu')\n",
    "            \n",
    "            conditioning_nn_dtype = next(self.conditioning_embedding.parameters()).dtype\n",
    "\n",
    "            if  conditioning_nn!= current_device or  conditioning_nn_dtype != current_dtype:\n",
    "                \n",
    "                self.conditioning_embedding.cast_model_to(device=current_device, dtype=current_dtype)\n",
    "                \n",
    "            \n",
    "        if controlnet_condition is not None:\n",
    "            # Check if it has the same shape as the sample otherwise error\n",
    "            # To the device of the sample\n",
    "            controlnet_condition = controlnet_condition.to(sample.device, dtype=sample.dtype)\n",
    "            controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n",
    "            controlnet_condition = controlnet_condition.flatten(0, 1)\n",
    "            if controlnet_condition.shape != sample.shape:\n",
    "                raise ValueError(f\"Jappie Controlnet condition shape {controlnet_condition.shape} does not match the sample shape {sample.shape}\")\n",
    "        else:\n",
    "            controlnet_condition = torch.zeros_like(sample).to(sample.device, dtype=sample.dtype)\n",
    "        \n",
    "        sample += controlnet_condition\n",
    "        \n",
    "\n",
    "        image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)\n",
    "\n",
    "        down_block_res_samples = (sample,)\n",
    "        \n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    image_only_indicator=image_only_indicator,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    image_only_indicator=image_only_indicator,\n",
    "                )\n",
    "            # Print the shapes of the res_samples\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.mid_block(\n",
    "            hidden_states=sample,\n",
    "        temb=emb,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            image_only_indicator=image_only_indicator,\n",
    "        )\n",
    "\n",
    "        \n",
    "        # 5. Control net blocks\n",
    "\n",
    "        # initialize the controlnet_down_block_res_samples of it is on embpy nn.ModuleList\n",
    "        if self.controlnet_down_blocks is None:\n",
    "            self.controlnet_down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "            for down_block_res_sample in down_block_res_samples:\n",
    "                # Determine the current number of channels in the tensor\n",
    "                current_channels = down_block_res_sample.size(1)\n",
    "                \n",
    "                # Dynamically create a zero convolution block for the current tensor\n",
    "                controlnet_block = nn.Conv2d(current_channels, current_channels, kernel_size=1)\n",
    "                controlnet_block = zero_module(controlnet_block).to(down_block_res_sample.device, dtype=sample.dtype)\n",
    "            \n",
    "                \n",
    "                # Store the processed sample for further use\n",
    "                self.controlnet_down_blocks.append(controlnet_block)\n",
    "    \n",
    "\n",
    "        controlnet_down_block_res_samples = ()\n",
    "\n",
    "        for index , (down_block_res_sample, controlnet_block) in enumerate(zip(down_block_res_samples, self.controlnet_down_blocks)):\n",
    "\n",
    "            # print to the debug console the device where the down_block_res_sample is\n",
    "            try:\n",
    "                # print the size of the down_block_res_sample\n",
    "                # print(f\"Down block res sample shape before the conversion: {down_block_res_sample.shape}\")\n",
    "                down_block_res_sample = controlnet_block(down_block_res_sample)\n",
    "                controlnet_down_block_res_samples = controlnet_down_block_res_samples + (down_block_res_sample,)\n",
    "            except Exception as e:\n",
    "                # Print the error in conjecuntion with the index\n",
    "                print(f\"Error at index {index}: {e}\")\n",
    "\n",
    "        down_block_res_samples = controlnet_down_block_res_samples\n",
    "\n",
    "        mid_block_res_sample = self.controlnet_mid_block(sample)\n",
    "\n",
    "        down_block_res_samples = [sample for sample in down_block_res_samples]\n",
    "        mid_block_res_sample = mid_block_res_sample\n",
    "\n",
    "        if not return_dict:\n",
    "            return (down_block_res_samples, mid_block_res_sample)\n",
    "\n",
    "        return SpatioTemporalControlNetOutput(\n",
    "            down_block_res_samples=down_block_res_samples, mid_block_res_sample=mid_block_res_sample\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_unet(\n",
    "        cls,\n",
    "        unet: UNetSpatioTemporalConditionModel,\n",
    "        load_weights_from_unet: bool = True,\n",
    "    ):\n",
    "        \n",
    "        addition_time_embed_dim = (\n",
    "            unet.config.addition_time_embed_dim if \"addition_time_embed_dim\" in unet.config else None\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # conditioning net\n",
    "        condition_net = CustomConditioningNet()\n",
    "\n",
    "        controlnet = cls(\n",
    "            in_channels=unet.config.in_channels,\n",
    "            down_block_types=unet.config.down_block_types,\n",
    "            block_out_channels=unet.config.block_out_channels,  # What are block out channels\n",
    "            addition_time_embed_dim=addition_time_embed_dim,\n",
    "            projection_class_embeddings_input_dim=unet.config.projection_class_embeddings_input_dim,\n",
    "            layers_per_block=unet.config.layers_per_block,\n",
    "            cross_attention_dim=unet.config.cross_attention_dim,\n",
    "            transformer_layers_per_block=unet.config.transformer_layers_per_block,\n",
    "            num_attention_heads=unet.config.num_attention_heads,\n",
    "            conditioning_embedding = condition_net\n",
    "        )\n",
    "\n",
    "        if load_weights_from_unet:\n",
    "            controlnet.conv_in.load_state_dict(unet.conv_in.state_dict())\n",
    "            controlnet.time_proj.load_state_dict(unet.time_proj.state_dict())\n",
    "            controlnet.time_embedding.load_state_dict(unet.time_embedding.state_dict())\n",
    "\n",
    "            controlnet.down_blocks.load_state_dict(unet.down_blocks.state_dict())\n",
    "            controlnet.mid_block.load_state_dict(unet.mid_block.state_dict())\n",
    "\n",
    "        return controlnet\n",
    "\n",
    "def zero_module(module):\n",
    "    for p in module.parameters():\n",
    "        nn.init.zeros_(p)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CustomConditioningNet' from 'diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet' (/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion_with_controlnet.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize the contrl net from my_net\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstable_video_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_stable_video_diffusion_with_controlnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableVideoDiffusionPipelineWithControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CustomConditioningNet' from 'diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet' (/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion_with_controlnet.py)"
     ]
    }
   ],
   "source": [
    "# initialize the contrl net from my_net\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "tokenizer = pipeline.tokenizer\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "del pipeline\n",
    "gc.collect() \n",
    "\n",
    "pipe_config = pipe.config\n",
    "print(pipe_config)\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "my_net = UNetSpatioTemporalConditionModel()\n",
    "my_net.load_state_dict(unet_weights)\n",
    "control_net = SpatioTemporalControlNet.from_unet(my_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    unet=my_net,\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    controlnet=control_net,\n",
    "    tokenizer = tokenizer,\n",
    "    text_encoder = text_encoder\n",
    ")\n",
    "pipe_with_controlnet.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/tmp/ipykernel_10502/1554748600.py:373: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  if next(self.conditioning_embedding.parameters()).is_cuda:\n",
      "/tmp/ipykernel_10502/1554748600.py:374: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  conditioning_nn = next(self.conditioning_embedding.parameters()).device\n",
      "/tmp/ipykernel_10502/1554748600.py:378: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  conditioning_nn_dtype = next(self.conditioning_embedding.parameters()).dtype\n",
      "/tmp/ipykernel_10502/1554748600.py:389: FutureWarning: Accessing config attribute `conditioning_embedding` directly via 'SpatioTemporalControlNet' object attribute is deprecated. Please access 'conditioning_embedding' over 'SpatioTemporalControlNet's config object instead, e.g. 'unet.config.conditioning_embedding'.\n",
      "  controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:04<01:57,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:08<01:36,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:12<01:27,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:16<01:21,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:19<01:16,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:23<01:11,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:27<01:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:30<01:04,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:34<01:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:38<00:56,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:42<00:52,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [00:45<00:48,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:49<00:45,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:53<00:41,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:57<00:37,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [01:01<00:34,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [01:04<00:30,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [01:08<00:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [01:12<00:22,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [01:16<00:19,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [01:20<00:15,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [01:23<00:11,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [01:27<00:07,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [01:31<00:03,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:35<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"d\"\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "# Define a simple torch generator\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "image = load_image(\"/home/wisley/custom_diffusers_library/frame2.png\")\n",
    "frames = pipe_with_controlnet(image = image, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajavi.mp4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(frames, \"jajavi.mp4\", fps=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input dtype: torch.float16\n",
      "Hidden image embeddings dtype: torch.float16\n",
      "torch.Size([2, 1, 8, 64, 64])\n",
      "torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 3])\n",
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def prepare_latents(\n",
    "    batch_size,\n",
    "    num_frames,\n",
    "    num_channels_latents,\n",
    "    height,\n",
    "    width,\n",
    "    dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents=None,\n",
    "):\n",
    "    shape = (\n",
    "        batch_size,\n",
    "        num_frames,\n",
    "        num_channels_latents // 2,\n",
    "        height // 1,\n",
    "        width // 1,\n",
    "    )\n",
    "    if isinstance(generator, list) and len(generator) != batch_size:\n",
    "        raise ValueError(\n",
    "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "        )\n",
    "\n",
    "    if latents is None:\n",
    "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "    else:\n",
    "        latents = latents.to(device)\n",
    "\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * 0.2\n",
    "    return latents\n",
    "\n",
    "def pseudo_image_embeddings( shape, generator, device, dtype, do_classifier_free_guidance = True ):\n",
    "    image_embeddings = randn_tensor(shape, generator=generator, device= device, dtype=dtype)\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        return torch.cat([negative_image_embeddings, image_embeddings])\n",
    "\n",
    "def get_add_time_ids(\n",
    "  fps = 7,\n",
    "  motion_bucket_id = 127,\n",
    "  noise_aug_strength = 0.02,\n",
    "  dtype = torch.float32,\n",
    "  batch_size = 1,\n",
    "  num_videos_per_prompt = 1,\n",
    "  do_classifier_free_guidance = True,\n",
    "):\n",
    "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "  if do_classifier_free_guidance:\n",
    "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "  return add_time_ids\n",
    "\n",
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Generate original latents with specified dtype\n",
    "my_latents = prepare_latents(1, 1, 8, 64, 64, dtype, device, generator)\n",
    "\n",
    "# Apply classifier-free guidance by duplicating the latents and ensuring the correct dtype\n",
    "latent_model_input = torch.cat([my_latents] * 2)  # Inherits dtype from my_latents\n",
    "\n",
    "# Create pseudo image latents by cloning the original latents\n",
    "pseudo_image_latents = latent_model_input.clone()  # Inherits dtype\n",
    "\n",
    "# Concatenate pseudo image latents over the channels dimension, ensuring matching dtype\n",
    "latent_model_input = torch.cat([latent_model_input, pseudo_image_latents], dim=2)\n",
    "\n",
    "\n",
    "# Create the fake image embeddings with the specified dtype\n",
    "hidden_image_embeddings = pseudo_image_embeddings((1, 1, 1024), generator, device, dtype)\n",
    "\n",
    "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
    "\n",
    "# Verify the dtype of both tensors\n",
    "print(f\"Latent model input dtype: {latent_model_input.dtype}\")\n",
    "print(f\"Hidden image embeddings dtype: {hidden_image_embeddings.dtype}\")\n",
    "\n",
    "print(latent_model_input.shape)\n",
    "print(hidden_image_embeddings.shape)\n",
    "print(added_time_ids.shape)\n",
    "\n",
    "# Print on which model they are\n",
    "print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "# Assuming added_time_ids is also a tensor; replace this with the actual tensor variable if different\n",
    "print(f\"Added time IDs are on: {added_time_ids.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n",
      "torch.Size([2, 1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        down_block_additional_residuals= None,\n",
    "        mid_block_additional_residual = None,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    print(noise_pred.shape)\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    do_classifier_free_guidance,\n",
    "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
    "    num_images_per_prompt = 1,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    text_encoder = None, \n",
    "    tokenizer = None):\n",
    "    # Set the text_encoder and the tokenizer on the correct device  \n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if True:\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "            text_input_ids, untruncated_ids\n",
    "        ):\n",
    "            removed_text = tokenizer.batch_decode(\n",
    "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "            )\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = text_inputs.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "       \n",
    "\n",
    "    if text_encoder is not None:\n",
    "        prompt_embeds_dtype = text_encoder.dtype\n",
    "\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
    "    \n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        uncond_tokens: List[str]\n",
    "        if negative_prompt is None:\n",
    "            uncond_tokens = [\"\"] * batch_size\n",
    "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "            raise TypeError(\n",
    "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                f\" {type(prompt)}.\"\n",
    "            )\n",
    "        elif isinstance(negative_prompt, str):\n",
    "            uncond_tokens = [negative_prompt]\n",
    "        elif batch_size != len(negative_prompt):\n",
    "            raise ValueError(\n",
    "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                \" the batch size of `prompt`.\"\n",
    "            )\n",
    "        else:\n",
    "            uncond_tokens = negative_prompt\n",
    "        \n",
    "        max_length = prompt_embeds.shape[1]\n",
    "        uncond_input = tokenizer(\n",
    "            uncond_tokens,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = uncond_input.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        negative_prompt_embeds = text_encoder(\n",
    "            uncond_input.input_ids.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "\n",
    "    \n",
    "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
    "    embeds = embeds.mean(dim=1, keepdim=True)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "prompt = \"A f a cat\"\n",
    "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "print(prompt_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n",
      "Time is on: cpu\n",
      "Time is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    time =  torch.tensor(1).to(dtype=dtype)\n",
    "\n",
    "    # print on which device the input is\n",
    "    print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "    print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "    print(f\"Added time IDs are on: {added_time_ids.device}\")\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "\n",
    "    # move time to divice\n",
    "    time = time.to(device)\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "           \n",
    "    noise_pred = control_net.forward(\n",
    "        torch.ones(2, 2, 8, 64, 64).to(dtype=dtype, device=device),\n",
    "        time,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        encoder_hidden_states = prompt_embeds.to(dtype=dtype),\n",
    "        # encoder_hidden_states = None,\n",
    "        return_dict=True,\n",
    "        # controlnet_condition = torch.ones(25, 4, 576, 1024).to(dtype=dtype, device=device)\n",
    "    )    \n",
    "    \n",
    "\n",
    "    # Print the sizes of the tensors\n",
    " \n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of down_block_res_samples afterprocesssing: 12\n",
      "This is the batch size 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    (down_block_res_samples, mid_block_res_samples) = control_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        time,\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    # reverse the down_block_res_samples tuple\n",
    "    # down_block_res_samples = down_block_res_samples[::-1]\n",
    "\n",
    "    print(f\"Length of down_block_res_samples afterprocesssing: {len(down_block_res_samples)}\")\n",
    "\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        # Maybe I need to reverse the order of the tensors\n",
    "        down_block_additional_residuals= down_block_res_samples,\n",
    "        mid_block_additional_residual = mid_block_res_samples,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    # Print the sizes of the tensors\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 320, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "conditioning_net = CustomConditioningNet()\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "hoi = conditioning_net.forward(pseudo_sample)\n",
    "print(hoi.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edit_diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
