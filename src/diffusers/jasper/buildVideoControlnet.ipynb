{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "\n",
    "\n",
    "from types import MethodType\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'num_frames': 2} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  7.43it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\", num_frames = 2\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlnet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConditioningNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        # Initial convolution to match the first target channel dimension\n",
    "        self.initial_conv = nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Defining a series of convolutional blocks to progressively downsample\n",
    "        # and increase channel dimensions towards the target size\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 320, kernel_size=3, stride=2, padding=1),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Final adjustment to target spatial dimensions\n",
    "        # Considering a final adaptive pooling layer to ensure matching to the target spatial size (64x64)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((72, 128))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x = torch.cat([x,x]) \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class UNetSpatioTemporalConditionOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    The output of [`UNetSpatioTemporalConditionModel`].\n",
    "\n",
    "    Args:\n",
    "        sample (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
    "            The hidden states output conditioned on `encoder_hidden_states` input. Output of last layer of model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sample: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class SpatioTemporalControlNetOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    The output of [`ControlNetModel`].\n",
    "\n",
    "    Args:\n",
    "        down_block_res_samples (`tuple[torch.Tensor]`):\n",
    "            A tuple of downsample activations at different resolutions for each downsampling block. Each tensor should\n",
    "            be of shape `(batch_size, channel * resolution, height //resolution, width // resolution)`. Output can be\n",
    "            used to condition the original UNet's downsampling activations.\n",
    "        mid_down_block_re_sample (`torch.Tensor`):\n",
    "            The activation of the midde block (the lowest sample resolution). Each tensor should be of shape\n",
    "            `(batch_size, channel * lowest_resolution, height // lowest_resolution, width // lowest_resolution)`.\n",
    "            Output can be used to condition the original UNet's middle block activation.\n",
    "    \"\"\"\n",
    "\n",
    "    down_block_res_samples: Tuple[torch.Tensor]\n",
    "    mid_block_res_sample: torch.Tensor\n",
    "    \n",
    "    # Add a class which prints the sizes of the tensors\n",
    "    def print_sizes(self):\n",
    "        print(f\"Down block res samples: {self.down_block_res_samples[0].shape}\")\n",
    "        print(f\"Mid block res sample: {self.mid_block_res_sample.shape}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SpatioTemporalControlNet(ModelMixin, ConfigMixin):\n",
    "    \"\"\"\n",
    "    A SpatioTemporalControlNet model for conditioning on spatio-temporal data.\n",
    "    This model adapts concepts from both ControlNetModel and UNetSpatioTemporalConditionModel,\n",
    "    focusing on handling video frames over time.\n",
    "    \"\"\"\n",
    "\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: Optional[int] = None,\n",
    "        in_channels: int = 8,\n",
    "        down_block_types: Tuple[str] = (\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"CrossAttnDownBlockSpatioTemporal\",\n",
    "            \"DownBlockSpatioTemporal\",\n",
    "        ),\n",
    "        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n",
    "        addition_time_embed_dim: int = 256,\n",
    "        projection_class_embeddings_input_dim: int = 768,\n",
    "        layers_per_block: Union[int, Tuple[int]] = 2,\n",
    "        cross_attention_dim: Union[int, Tuple[int]] = 1024,\n",
    "        transformer_layers_per_block: Union[int, Tuple[int], Tuple[Tuple]] = 1,\n",
    "        num_attention_heads: Union[int, Tuple[int]] = (5, 10, 10, 20),\n",
    "        conditioning_embedding = None\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.conditioning_embedding = conditioning_embedding\n",
    "\n",
    "        # input\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels,\n",
    "            block_out_channels[0],\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # time\n",
    "        time_embed_dim = block_out_channels[0] * 4\n",
    "\n",
    "        self.time_proj = Timesteps(block_out_channels[0], True, downscale_freq_shift=0)\n",
    "        timestep_input_dim = block_out_channels[0]\n",
    "\n",
    "        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "\n",
    "        self.add_time_proj = Timesteps(addition_time_embed_dim, True, downscale_freq_shift=0)\n",
    "        self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)\n",
    "\n",
    "\n",
    "        output_channel = block_out_channels[0]\n",
    "        \n",
    "        self.controlnet_down_blocks = None\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "        \n",
    "        if isinstance(num_attention_heads, int):\n",
    "            num_attention_heads = (num_attention_heads,) * len(down_block_types)\n",
    "\n",
    "        if isinstance(cross_attention_dim, int):\n",
    "            cross_attention_dim = (cross_attention_dim,) * len(down_block_types)\n",
    "\n",
    "        if isinstance(layers_per_block, int):\n",
    "            layers_per_block = [layers_per_block] * len(down_block_types)\n",
    "\n",
    "        if isinstance(transformer_layers_per_block, int):\n",
    "            transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n",
    "\n",
    "        blocks_time_embed_dim = time_embed_dim\n",
    "\n",
    "        # Initialize the connection between the down blocks and the unet\n",
    "        output_channel = block_out_channels[0]\n",
    "\n",
    "        # controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "        # controlnet_block = zero_module(controlnet_block)\n",
    "        # self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "        # down\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                temb_channels=blocks_time_embed_dim,\n",
    "                num_layers=layers_per_block[i],\n",
    "                transformer_layers_per_block=transformer_layers_per_block[i],\n",
    "                add_downsample= not is_final_block,\n",
    "                resnet_eps=1e-5,\n",
    "                down_block_type=down_block_type,\n",
    "                cross_attention_dim=cross_attention_dim[i],\n",
    "                num_attention_heads=num_attention_heads[i],\n",
    "                resnet_act_fn=\"silu\",\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "            # for _ in range(layers_per_block[i]):\n",
    "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "            #     controlnet_block = zero_module(controlnet_block)\n",
    "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "            # if not is_final_block:\n",
    "            #     controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)\n",
    "            #     controlnet_block = zero_module(controlnet_block)\n",
    "            #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # hardcoded_controlnet_block_dims = [320,320, 640,640, 1280, 1280, 1280,1280,1280]\n",
    "        # for index, controlnet_block_dim in enumerate(hardcoded_controlnet_block_dims):\n",
    "        #     controlnet_block = nn.Conv2d(controlnet_block_dim, controlnet_block_dim, kernel_size=1)\n",
    "        #     controlnet_block = zero_module(controlnet_block)\n",
    "        #     self.controlnet_down_blocks.append(controlnet_block)\n",
    "\n",
    "\n",
    "        # Connections for the mid block\n",
    "        mid_block_channel = block_out_channels[-1]\n",
    "\n",
    "        controlnet_block = nn.Conv2d(mid_block_channel, mid_block_channel, kernel_size=1)\n",
    "        controlnet_block = zero_module(controlnet_block)\n",
    "        self.controlnet_mid_block = controlnet_block\n",
    "\n",
    "\n",
    "        # mid\n",
    "        self.mid_block = UNetMidBlockSpatioTemporal(\n",
    "            block_out_channels[-1],\n",
    "            temb_channels=blocks_time_embed_dim,\n",
    "            transformer_layers_per_block=transformer_layers_per_block[-1],\n",
    "            cross_attention_dim=cross_attention_dim[-1],\n",
    "            num_attention_heads=num_attention_heads[-1],\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        added_time_ids: torch.Tensor,\n",
    "        return_dict: bool = True,\n",
    "        controlnet_condition : torch.FloatTensor = None,\n",
    "    ) -> Union[UNetSpatioTemporalConditionOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNetSpatioTemporalConditionModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`):\n",
    "                The noisy input tensor with the following shape `(batch, num_frames, channel, height, width)`.\n",
    "            timestep (`torch.FloatTensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                The encoder hidden states with shape `(batch, sequence_length, cross_attention_dim)`.\n",
    "            added_time_ids: (`torch.FloatTensor`):\n",
    "                The additional time ids with shape `(batch, num_additional_ids)`. These are encoded with sinusoidal\n",
    "                embeddings and added to the time embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] instead of a plain\n",
    "                tuple.\n",
    "        Returns:\n",
    "            [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unet_slatio_temporal.UNetSpatioTemporalConditionOutput`] is returned, otherwise\n",
    "                a `tuple` is returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size, num_frames = sample.shape[:2]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        time_embeds = self.add_time_proj(added_time_ids.flatten())\n",
    "        time_embeds = time_embeds.reshape((batch_size, -1))\n",
    "        time_embeds = time_embeds.to(emb.dtype)\n",
    "        aug_emb = self.add_embedding(time_embeds)\n",
    "        emb = emb + aug_emb\n",
    "\n",
    "        # Flatten the batch and frames dimensions\n",
    "        # sample: [batch, frames, channels, height, width] -> [batch * frames, channels, height, width]\n",
    "        sample = sample.flatten(0, 1)\n",
    "\n",
    "        \n",
    "        # Repeat the embeddings num_video_frames times\n",
    "        # emb: [batch, channels] -> [batch * frames, channels]\n",
    "        emb = emb.repeat_interleave(num_frames, dim=0).to(sample.device)\n",
    "        # encoder_hidden_states: [batch, 1, channels] -> [batch * frames, 1, channels]\n",
    "        # Let encoder_hidden_states be just zeros in the correct format\n",
    "        # shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
    "\n",
    "        \n",
    "        current_device = self._execution_device\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            shape_encoder_hidden_states = (batch_size * num_frames, 1, 1024)\n",
    "            encoder_hidden_states = torch.zeros(shape_encoder_hidden_states, device=sample.device).repeat_interleave(num_frames, dim=0).to(dtype=sample.dtype)\n",
    "            print(f\"Shape of encoder hidden states without: {encoder_hidden_states.shape}\")\n",
    "        else: \n",
    "            encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=0)\n",
    "            print(f\"Shape of encoder hidden states with: {encoder_hidden_states.shape}\")\n",
    "        \n",
    "        # Print the shape of the sample\n",
    "        # 2. pre-process\n",
    "        print(f\"Sample shape before the conversion: {sample.shape}\")\n",
    "        sample = self.conv_in(sample)\n",
    "        print(f\"Sample shape after the conversion: {sample.shape}\")\n",
    "\n",
    "\n",
    "        # Make sure the controlnet_condition model and the controlet have if same type\n",
    "        # And are running on the same device\n",
    "\n",
    "        if controlnet_condition is not None:  \n",
    "            if self.conditioning_embedding.device != current_device or self.conditioning_embedding.dtype != current_dtype:\n",
    "                self.conditioning_embedding.to(device=current_device, dtype=current_dtype)\n",
    "            \n",
    "        if controlnet_condition is not None:\n",
    "            # Check if it has the same shape as the sample otherwise error\n",
    "            # To the device of the sample\n",
    "            controlnet_condition =  self.conditioning_embedding.forward(controlnet_condition)\n",
    "            controlnet_condition = controlnet_condition.flatten(0, 1)\n",
    "            if controlnet_condition.shape != sample.shape:\n",
    "                raise ValueError(f\"Jappie Controlnet condition shape {controlnet_condition.shape} does not match the sample shape {sample.shape}\")\n",
    "        else:\n",
    "            controlnet_condition = torch.zeros_like(sample).to(sample.device, dtype=sample.dtype)\n",
    "        \n",
    "        sample += controlnet_condition\n",
    "        \n",
    "\n",
    "        image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)\n",
    "\n",
    "        down_block_res_samples = (sample,)\n",
    "        \n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    image_only_indicator=image_only_indicator,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    image_only_indicator=image_only_indicator,\n",
    "                )\n",
    "            # Print the shapes of the res_samples\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.mid_block(\n",
    "            hidden_states=sample,\n",
    "        temb=emb,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            image_only_indicator=image_only_indicator,\n",
    "        )\n",
    "\n",
    "        \n",
    "        # 5. Control net blocks\n",
    "\n",
    "        # initialize the controlnet_down_block_res_samples of it is on embpy nn.ModuleList\n",
    "        if self.controlnet_down_blocks is None:\n",
    "            self.controlnet_down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "            for down_block_res_sample in down_block_res_samples:\n",
    "                # Determine the current number of channels in the tensor\n",
    "                current_channels = down_block_res_sample.size(1)\n",
    "                \n",
    "                # Dynamically create a zero convolution block for the current tensor\n",
    "                controlnet_block = nn.Conv2d(current_channels, current_channels, kernel_size=1)\n",
    "                controlnet_block = zero_module(controlnet_block).to(down_block_res_sample.device, dtype=sample.dtype)\n",
    "            \n",
    "                \n",
    "                # Store the processed sample for further use\n",
    "                self.controlnet_down_blocks.append(controlnet_block)\n",
    "    \n",
    "\n",
    "        controlnet_down_block_res_samples = ()\n",
    "\n",
    "        for index , (down_block_res_sample, controlnet_block) in enumerate(zip(down_block_res_samples, self.controlnet_down_blocks)):\n",
    "\n",
    "            # print to the debug console the device where the down_block_res_sample is\n",
    "            try:\n",
    "                # print the size of the down_block_res_sample\n",
    "                print(f\"Down block res sample shape before the conversion: {down_block_res_sample.shape}\")\n",
    "                down_block_res_sample = controlnet_block(down_block_res_sample)\n",
    "                controlnet_down_block_res_samples = controlnet_down_block_res_samples + (down_block_res_sample,)\n",
    "            except Exception as e:\n",
    "                # Print the error in conjecuntion with the index\n",
    "                print(f\"Error at index {index}: {e}\")\n",
    "\n",
    "        down_block_res_samples = controlnet_down_block_res_samples\n",
    "\n",
    "        mid_block_res_sample = self.controlnet_mid_block(sample)\n",
    "\n",
    "        down_block_res_samples = [sample for sample in down_block_res_samples]\n",
    "        mid_block_res_sample = mid_block_res_sample\n",
    "\n",
    "        if not return_dict:\n",
    "            return (down_block_res_samples, mid_block_res_sample)\n",
    "\n",
    "        return SpatioTemporalControlNetOutput(\n",
    "            down_block_res_samples=down_block_res_samples, mid_block_res_sample=mid_block_res_sample\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_unet(\n",
    "        cls,\n",
    "        unet: UNetSpatioTemporalConditionModel,\n",
    "        load_weights_from_unet: bool = True,\n",
    "    ):\n",
    "        \n",
    "        addition_time_embed_dim = (\n",
    "            unet.config.addition_time_embed_dim if \"addition_time_embed_dim\" in unet.config else None\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # conditioning net\n",
    "        condition_net = CustomConditioningNet()\n",
    "\n",
    "        controlnet = cls(\n",
    "            in_channels=unet.config.in_channels,\n",
    "            down_block_types=unet.config.down_block_types,\n",
    "            block_out_channels=unet.config.block_out_channels,  # What are block out channels\n",
    "            addition_time_embed_dim=addition_time_embed_dim,\n",
    "            projection_class_embeddings_input_dim=unet.config.projection_class_embeddings_input_dim,\n",
    "            layers_per_block=unet.config.layers_per_block,\n",
    "            cross_attention_dim=unet.config.cross_attention_dim,\n",
    "            transformer_layers_per_block=unet.config.transformer_layers_per_block,\n",
    "            num_attention_heads=unet.config.num_attention_heads,\n",
    "            conditioning_embedding = condition_net\n",
    "        )\n",
    "\n",
    "        if load_weights_from_unet:\n",
    "            controlnet.conv_in.load_state_dict(unet.conv_in.state_dict())\n",
    "            controlnet.time_proj.load_state_dict(unet.time_proj.state_dict())\n",
    "            controlnet.time_embedding.load_state_dict(unet.time_embedding.state_dict())\n",
    "\n",
    "            controlnet.down_blocks.load_state_dict(unet.down_blocks.state_dict())\n",
    "            controlnet.mid_block.load_state_dict(unet.mid_block.state_dict())\n",
    "\n",
    "        return controlnet\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    for p in module.parameters():\n",
    "        nn.init.zeros_(p)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 17.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "tokenizer = pipeline.tokenizer\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "del pipeline\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the contrl net from my_net\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet\n",
    "pipe_config = pipe.config\n",
    "print(pipe_config)\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "my_net = UNetSpatioTemporalConditionModel()\n",
    "my_net.load_state_dict(unet_weights)\n",
    "control_net = SpatioTemporalControlNet.from_unet(my_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    unet=my_net,\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    controlnet=control_net,\n",
    "    tokenizer = tokenizer,\n",
    "    text_encoder = text_encoder\n",
    ")\n",
    "pipe_with_controlnet.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pseudo_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m578\u001b[39m, \u001b[38;5;241m1028\u001b[39m)\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/wisley/diffusers/pca.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mpipe_with_controlnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditioning_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpseudo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mframes[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion_with_controlnet.py:605\u001b[0m, in \u001b[0;36mStableVideoDiffusionPipelineWithControlNet.__call__\u001b[0;34m(self, image, prompt, conditioning_image, height, width, num_frames, num_inference_steps, min_guidance_scale, max_guidance_scale, fps, motion_bucket_id, noise_aug_strength, decode_chunk_size, num_videos_per_prompt, generator, latents, output_type, callback_on_step_end, callback_on_step_end_tensor_inputs, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_upcasting:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 605\u001b[0m image_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_vae_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m image_latents \u001b[38;5;241m=\u001b[39m image_latents\u001b[38;5;241m.\u001b[39mto(image_embeddings\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# cast back to fp16 if needed\u001b[39;00m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion_with_controlnet.py:319\u001b[0m, in \u001b[0;36mStableVideoDiffusionPipelineWithControlNet._encode_vae_image\u001b[0;34m(self, image, device, num_videos_per_prompt, do_classifier_free_guidance)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_vae_image\u001b[39m(\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    313\u001b[0m     image: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     do_classifier_free_guidance,\n\u001b[1;32m    317\u001b[0m ):\n\u001b[1;32m    318\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 319\u001b[0m     image_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlatent_dist\u001b[38;5;241m.\u001b[39mmode()\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n\u001b[1;32m    322\u001b[0m         negative_image_latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(image_latents)\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py:332\u001b[0m, in \u001b[0;36mAutoencoderKLTemporalDecoder.encode\u001b[0;34m(self, x, return_dict)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;129m@apply_forward_hook\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mFloatTensor, return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    319\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:\n\u001b[1;32m    320\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    Encode a batch of images into latents.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m            [`~models.autoencoder_kl.AutoencoderKLOutput`] is returned, otherwise a plain `tuple` is returned.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     moments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[1;32m    334\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/autoencoders/vae.py:172\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# down\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[0;32m--> 172\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample)\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_blocks.py:1381\u001b[0m, in \u001b[0;36mDownEncoderBlock2D.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m downsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers:\n\u001b[0;32m-> 1381\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdownsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/downsampling.py:148\u001b[0m, in \u001b[0;36mDownsample2D.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv, LoRACompatibleConv):\n\u001b[0;32m--> 148\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(hidden_states)\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/lora.py:358\u001b[0m, in \u001b[0;36mLoRACompatibleConv.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, scale: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;66;03m# make sure to the functional Conv2D function as otherwise torch.compile's graph will break\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;66;03m# see: https://github.com/huggingface/diffusers/pull/4315\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m         original_outputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    363\u001b[0m             hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    364\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"d\"\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "\n",
    "image = load_image(\"/home/wisley/diffusers/pca.png\")\n",
    "frames = pipe_with_controlnet(image = image, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input dtype: torch.float16\n",
      "Hidden image embeddings dtype: torch.float16\n",
      "torch.Size([2, 1, 8, 64, 64])\n",
      "torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 3])\n",
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def prepare_latents(\n",
    "    batch_size,\n",
    "    num_frames,\n",
    "    num_channels_latents,\n",
    "    height,\n",
    "    width,\n",
    "    dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents=None,\n",
    "):\n",
    "    shape = (\n",
    "        batch_size,\n",
    "        num_frames,\n",
    "        num_channels_latents // 2,\n",
    "        height // 1,\n",
    "        width // 1,\n",
    "    )\n",
    "    if isinstance(generator, list) and len(generator) != batch_size:\n",
    "        raise ValueError(\n",
    "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "        )\n",
    "\n",
    "    if latents is None:\n",
    "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "    else:\n",
    "        latents = latents.to(device)\n",
    "\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * 0.2\n",
    "    return latents\n",
    "\n",
    "def pseudo_image_embeddings( shape, generator, device, dtype, do_classifier_free_guidance = True ):\n",
    "    image_embeddings = randn_tensor(shape, generator=generator, device= device, dtype=dtype)\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        return torch.cat([negative_image_embeddings, image_embeddings])\n",
    "\n",
    "def get_add_time_ids(\n",
    "  fps = 7,\n",
    "  motion_bucket_id = 127,\n",
    "  noise_aug_strength = 0.02,\n",
    "  dtype = torch.float32,\n",
    "  batch_size = 1,\n",
    "  num_videos_per_prompt = 1,\n",
    "  do_classifier_free_guidance = True,\n",
    "):\n",
    "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "  if do_classifier_free_guidance:\n",
    "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "  return add_time_ids\n",
    "\n",
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Generate original latents with specified dtype\n",
    "my_latents = prepare_latents(1, 1, 8, 64, 64, dtype, device, generator)\n",
    "\n",
    "# Apply classifier-free guidance by duplicating the latents and ensuring the correct dtype\n",
    "latent_model_input = torch.cat([my_latents] * 2)  # Inherits dtype from my_latents\n",
    "\n",
    "# Create pseudo image latents by cloning the original latents\n",
    "pseudo_image_latents = latent_model_input.clone()  # Inherits dtype\n",
    "\n",
    "# Concatenate pseudo image latents over the channels dimension, ensuring matching dtype\n",
    "latent_model_input = torch.cat([latent_model_input, pseudo_image_latents], dim=2)\n",
    "\n",
    "\n",
    "# Create the fake image embeddings with the specified dtype\n",
    "hidden_image_embeddings = pseudo_image_embeddings((1, 1, 1024), generator, device, dtype)\n",
    "\n",
    "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
    "\n",
    "# Verify the dtype of both tensors\n",
    "print(f\"Latent model input dtype: {latent_model_input.dtype}\")\n",
    "print(f\"Hidden image embeddings dtype: {hidden_image_embeddings.dtype}\")\n",
    "\n",
    "print(latent_model_input.shape)\n",
    "print(hidden_image_embeddings.shape)\n",
    "print(added_time_ids.shape)\n",
    "\n",
    "# Print on which model they are\n",
    "print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "# Assuming added_time_ids is also a tensor; replace this with the actual tensor variable if different\n",
    "print(f\"Added time IDs are on: {added_time_ids.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch size 2\n",
      "This is the iteration 0\n",
      "This is the iteration 1\n",
      "This is the iteration 2\n",
      "This is the iteration 3\n",
      "torch.Size([2, 1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        down_block_additional_residuals= None,\n",
    "        mid_block_additional_residual = None,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    print(noise_pred.shape)\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    do_classifier_free_guidance,\n",
    "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
    "    num_images_per_prompt = 1,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    text_encoder = None, \n",
    "    tokenizer = None):\n",
    "    # Set the text_encoder and the tokenizer on the correct device  \n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if True:\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "            text_input_ids, untruncated_ids\n",
    "        ):\n",
    "            removed_text = tokenizer.batch_decode(\n",
    "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "            )\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = text_inputs.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "       \n",
    "\n",
    "    if text_encoder is not None:\n",
    "        prompt_embeds_dtype = text_encoder.dtype\n",
    "\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
    "    \n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        uncond_tokens: List[str]\n",
    "        if negative_prompt is None:\n",
    "            uncond_tokens = [\"\"] * batch_size\n",
    "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "            raise TypeError(\n",
    "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                f\" {type(prompt)}.\"\n",
    "            )\n",
    "        elif isinstance(negative_prompt, str):\n",
    "            uncond_tokens = [negative_prompt]\n",
    "        elif batch_size != len(negative_prompt):\n",
    "            raise ValueError(\n",
    "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                \" the batch size of `prompt`.\"\n",
    "            )\n",
    "        else:\n",
    "            uncond_tokens = negative_prompt\n",
    "        \n",
    "        max_length = prompt_embeds.shape[1]\n",
    "        uncond_input = tokenizer(\n",
    "            uncond_tokens,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = uncond_input.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        negative_prompt_embeds = text_encoder(\n",
    "            uncond_input.input_ids.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "\n",
    "    \n",
    "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
    "    embeds = embeds.mean(dim=1, keepdim=True)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "prompt = \"A f a cat\"\n",
    "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "print(prompt_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n",
      "Time is on: cpu\n",
      "Time is on: cuda:0\n",
      "Shape of encoder hidden states with: torch.Size([4, 1, 1024])\n",
      "Sample shape before the conversion: torch.Size([4, 8, 64, 64])\n",
      "Sample shape after the conversion: torch.Size([4, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 320, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 640, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 640, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 640, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 1280, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 1280, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 1280, 8, 8])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 1280, 8, 8])\n",
      "Down block res sample shape before the conversion: torch.Size([4, 1280, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    time =  torch.tensor(1).to(dtype=dtype)\n",
    "\n",
    "    # print on which device the input is\n",
    "    print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "    print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "    print(f\"Added time IDs are on: {added_time_ids.device}\")\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "\n",
    "    # move time to divice\n",
    "    time = time.to(device)\n",
    "    print(f\"Time is on: {time.device}\")\n",
    "           \n",
    "    noise_pred = control_net.forward(\n",
    "        torch.ones(2, 2, 8, 64, 64).to(dtype=dtype, device=device),\n",
    "        time,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        encoder_hidden_states = prompt_embeds.to(dtype=dtype),\n",
    "        # encoder_hidden_states = None,\n",
    "        return_dict=True,\n",
    "        # controlnet_condition = torch.ones(25, 4, 576, 1024).to(dtype=dtype, device=device)\n",
    "    )    \n",
    "    \n",
    "\n",
    "    # Print the sizes of the tensors\n",
    " \n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder hidden states with: torch.Size([2, 1, 1024])\n",
      "Sample shape before the conversion: torch.Size([2, 8, 64, 64])\n",
      "Sample shape after the conversion: torch.Size([2, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 320, 64, 64])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 320, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 640, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 640, 32, 32])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 640, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 1280, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 1280, 16, 16])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
      "Down block res sample shape before the conversion: torch.Size([2, 1280, 8, 8])\n",
      "Length of down_block_res_samples afterprocesssing: 12\n",
      "This is the batch size 2\n",
      "AAAAAAAAAAAAAAAAAA Encoder hidden states shape: torch.Size([2, 1, 1024])\n",
      "AAAAAAAAAAAAAAAAAA Sample shape: torch.Size([2, 320, 64, 64])\n",
      "Res samples shape unet: torch.Size([2, 320, 64, 64])\n",
      "Res samples shape unet: torch.Size([2, 640, 32, 32])\n",
      "Res samples shape unet: torch.Size([2, 1280, 16, 16])\n",
      "Res samples shape unet: torch.Size([2, 1280, 8, 8])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 320, 64, 64])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 320, 64, 64])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 320, 64, 64])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 320, 32, 32])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 640, 32, 32])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 640, 32, 32])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 640, 16, 16])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 1280, 16, 16])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 1280, 16, 16])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 1280, 8, 8])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 1280, 8, 8])\n",
      "Down block res samples shape uuuuuuuu: torch.Size([2, 1280, 8, 8])\n",
      "This is the iteration 0\n",
      "This is the iteration 1\n",
      "This is the iteration 2\n",
      "This is the iteration 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    (down_block_res_samples, mid_block_res_samples) = control_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        time,\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    # reverse the down_block_res_samples tuple\n",
    "    # down_block_res_samples = down_block_res_samples[::-1]\n",
    "\n",
    "    print(f\"Length of down_block_res_samples afterprocesssing: {len(down_block_res_samples)}\")\n",
    "\n",
    "\n",
    "    noise_pred = my_net.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        # Maybe I need to reverse the order of the tensors\n",
    "        down_block_additional_residuals= down_block_res_samples,\n",
    "        mid_block_additional_residual = mid_block_res_samples,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    # Print the sizes of the tensors\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 320, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "conditioning_net = CustomConditioningNet()\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "hoi = conditioning_net.forward(pseudo_sample)\n",
    "print(hoi.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edit_diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
